{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CVE Analysis Engine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement\n",
    "\n",
    "There are two sides to my problem:\n",
    "\n",
    "1. **Given input descriptions, predict the cvss vector.**\n",
    "   This is a multi-label, multi-class classification problem.\n",
    "   Some potential strategies are defined below.\n",
    "1. **Given input descriptions, suggest a cvss score directly.**\n",
    "   This is probably a regression problem, although it can\n",
    "   be converted into a classification problem with buckets\n",
    "   score buckets of some discrete size.\n",
    "\n",
    "- Independent labels: train a separate classifier for each label, probably using softmax regression\n",
    "- Dependent labels: classifier chains - input to a classifier includes output from another\n",
    "- Dependent labels: label powerset - transform problem into a multi-class problem\n",
    "  with one multi-class classifier is trained on all unique label combinations\n",
    "  found in the training data.  Deals efficiently with label correlations.\n",
    "\n",
    "I need to make a decision regarding the independence assumption of my labels.\n",
    "I find it intellectually interesting to explore the correlation statistics\n",
    "between the category + label combinations.  Two methods for establishing\n",
    "correlation between categories is\n",
    "- Chi-square test of independence\n",
    "- Cramer's V\n",
    "\n",
    "#### Next steps\n",
    "\n",
    "- Look at documentation to make sure I've got my problem statements right.\n",
    "  Does vector suggestion deliver value?\n",
    "- Perform a *Cramer's V* analysis on training examples\n",
    "- Based on the output of this, decide on ml strategy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding CVSS Vectors\n",
    "\n",
    "The following table lists out the metrics comprised by the CVSS vector.\n",
    "More info can be found in the [CVSS v3.1 Specification Document](https://www.first.org/cvss/specification-document).\n",
    "Each metric contributes a predefined amount to the overall CVE CVSS score,\n",
    "and the score is completely determined by the values of these metrics.\n",
    "Values in the table below are ordered from most to least severe.\n",
    "\n",
    "| **Base metric** | **Base metric type** | **Description** | **Possible values** |\n",
    "|---|---|---|---|\n",
    "| Attack Vector (AV) | Exploitability | This metric reflects the context by which vulnerability exploitation is possible. | Network, Adjacent, Local, Physical |\n",
    "| Attack Complexity (AC) | Exploitability | This metric describes the conditions beyond the attacker’s control that must exist in order to exploit the vulnerability. | Low, High |\n",
    "| Privileges Required (PR) | Exploitability | This metric describes the level of privileges an attacker must possess before successfully exploiting the vulnerability. | None, Low, High |\n",
    "| User Interaction (UI) | Exploitability | This metric captures the requirement for a human user, other than the attacker, to participate in the successful compromise of the vulnerable component. | None, Required |\n",
    "| Scope (S) | Scope¹ | The Scope metric captures whether a vulnerability in one vulnerable component impacts resources in components beyond its security scope. | Changed, Unchanged |\n",
    "| Confidentiality (C) | Impact | This metric measures the impact to the confidentiality of the information resources managed by a software component due to a successfully exploited vulnerability. | High, Low, None |\n",
    "| Integrity (I) | Impact | This metric measures the impact to integrity of a successfully exploited vulnerability. | High, Low, None |\n",
    "| Availability (A) | Impact | This metric measures the impact to the availability of the impacted component resulting from a successfully exploited vulnerability. | High, Low, None |\n",
    "\n",
    "<br>\n",
    "\n",
    "> ¹Scope was introduced in CVSS3.1.\n",
    "\n",
    "For our model, the precise meaning of these metrics and their subcategories is unimportant.\n",
    "The relevant question is: _how independent are these metrics and subcategories?_\n",
    "\n",
    "Assumption of label independence certainly makes our job easier,\n",
    "as it leaves the door open for more basic machine learning algorithms\n",
    "like **multi-category logistic regression**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1: Logistic Regression\n",
    "\n",
    "Logistic Regression is often referred to as the _discriminative_\n",
    "counterpart of Naive Bayes.\n",
    "\n",
    "Model $P(y | \\mathbf{x}_i)$ and assume it takes exactly the form\n",
    "\n",
    "$$\n",
    "    P(y | \\mathbf{x}_i) = \\frac{1}{1 + e^{-y(\\mathbf{w}^T\\mathbf{x}_i + b)}}\n",
    "$$\n",
    "\n",
    "while making few assumptions about $P(\\mathbf{x}_i | y)$.\n",
    "Ultimately it doesn't matter, because we estimate $\\mathbf{w}$ and $b$\n",
    "directly with MLE or MAP to maximize the conditional likelihood of\n",
    "\n",
    "$$\n",
    "    \\prod_i P(y_i | \\mathbf{x}_i; \\mathbf{w}, b)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE\n",
    "\n",
    "Choose parameters that maximize the conditional likelihood.\n",
    "The conditional data likelihood $P(\\mathbf{y} | \\mathbf{X}, \\mathbf{w})$\n",
    "is the probability of the observed values $\\mathbf{y} \\in \\mathbb{R}^n$\n",
    "in the training data conditioned on the feature values $\\mathbf{x}_i$.\n",
    "Note that $\\mathbf{X} = [\\mathbf{x}_1,\\dots,\\mathbf{x}_n] \\in \\mathbb{R}^{d \\times n}$.\n",
    "We choose the parameters that maximize this function, and we assume that\n",
    "the $y_i$ are independent given the input features $\\mathbf{x}_i$ and $\\mathbf{w}$.\n",
    "\n",
    "> In my view, for CVE vectors, this assumption is perfectly valid to make\n",
    "\n",
    "$$\n",
    "    P(\\mathbf{y} | \\mathbf{X}, \\mathbf{w}) = \\prod_{i=1}^{n} P(y_i | \\mathbf{x}_i, \\mathbf{w}) \\\\\n",
    "    \\hat{\\mathbf{w}}_{\\text{MLE}}\n",
    "    = \\underset{\\mathbf{w}}{\\arg\\max}\n",
    "    - \\sum_{i=1}^{n}\\log(1 + e^{-y_i\\mathbf{w}^T\\mathbf{x}_i}) \\\\\n",
    "    = \\underset{\\mathbf{w}}{\\arg\\min} \\sum_{i=1}^{n}\\log(1 + e^{-y_i\\mathbf{w}^T\\mathbf{x}_i})\n",
    "$$\n",
    "\n",
    "Use gradient descent on the _negative log likelihood_.\n",
    "\n",
    "$$\n",
    "    \\ell(\\mathbf{w}) = \\sum_{i=1}^{n}\\log(1 + e^{-y_i\\mathbf{w}^T\\mathbf{x}_i})\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing\n",
    "\n",
    "1. lowercase all text\n",
    "1. remove punctuation\n",
    "1. tokenize\n",
    "1. remove stop words\n",
    "1. lemmatization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = \"svg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cvss\n",
    "import cvss.exceptions\n",
    "import nltk\n",
    "import json\n",
    "import logging\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True, raise_on_error=True)\n",
    "nltk.download(\"wordnet\", quiet=True, raise_on_error=True)\n",
    "logging.getLogger(\"matplotlib.font_manager\").setLevel(logging.ERROR)\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"[%(levelname)-8s] (%(name)s) %(message)s\",\n",
    "    level=logging.DEBUG,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"../data/cve/cves.json\")\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calc_cvss_score(v: str) -> float:\n",
    "    try:\n",
    "        return cvss.CVSS3(v).scores()[0]\n",
    "    except cvss.exceptions.CVSS3MalformedError:\n",
    "        return -1.0\n",
    "\n",
    "\n",
    "df[\"parsed_scores\"] = df[\"XYZ_CVSS_VECTOR\"].dropna().apply(_calc_cvss_score)\n",
    "df[\"failed_to_parse\"] = df[\"XYZ_CVSS_SCORE\"].notna() & (df[\"XYZ_CVSS_SCORE\"] != df[\"parsed_scores\"])\n",
    "# df[[\"XYZ_CVSS_SCORE\", \"parsed_scores\", \"failed_to_parse\"]].to_csv(\"../unparseable_vectors.csv\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "1. parse cvss vector to columns like `AV`, `C`, etc\n",
    "1. preprocess descriptions into `processed_desc` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cve_engine.cvss_data import CVSS_BASE_METRICS\n",
    "from cve_engine.data_processing import (\n",
    "    clean_cvss_vector,\n",
    "    desc_preprocess,\n",
    "    vec_parse_metric,\n",
    "    create_bow,\n",
    ")\n",
    "\n",
    "\n",
    "def extract_cvss_vector_components(df: pd.DataFrame, vector: pd.Series):\n",
    "    for metric in CVSS_BASE_METRICS:\n",
    "        df[metric] = vector.dropna().apply(lambda v: vec_parse_metric(v, metric))\n",
    "    return df\n",
    "\n",
    "\n",
    "# process descriptions\n",
    "df[\"processed_desc\"] = df[\"DESCRIPTION\"].apply(desc_preprocess)\n",
    "# try to parse and clean up cvss vectors\n",
    "df[\"vector\"] = df.XYZ_CVSS_VECTOR.apply(clean_cvss_vector)\n",
    "\n",
    "df = extract_cvss_vector_components(df, df[\"vector\"])\n",
    "print(f\"rows with valid cvss vectors: {df['vector'].count()}\")\n",
    "\n",
    "df.to_csv(\"../df.csv\")\n",
    "# df[[\"processed_desc\", \"AV\"]].to_csv(\"../for_autogluon.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, X = create_bow(df[\"processed_desc\"].to_list())\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Statistical Data Analysis\n",
    "\n",
    "Below are some statistical analyses I performed\n",
    "on the data to get a sense for its characteristics\n",
    "\n",
    "Pearson's $\\Chi^2$ test\n",
    "\n",
    "- [Pearson's chi-squared test - Wikipedia](https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test)\n",
    "\n",
    "Based on the below analysis, a couple of the highest correlations:\n",
    "\n",
    "- AV:L & PR:L highly correlated\n",
    "- AV:N & PR:L highly negatively correlated\n",
    "- PR:L & UI:R highly negatively correlated\n",
    "- PR:N & UI:R highly correlated\n",
    "- PR:H & S:C highly correlated\n",
    "- I:L & S:C very highly correlated\n",
    "- A:N & S:C very highly correlated\n",
    "- C:* & I* extemely correlated both positively and negatively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import itertools\n",
    "\n",
    "crosstabs = {}\n",
    "\n",
    "\n",
    "def perform_independence_test(df: pd.DataFrame):\n",
    "    for c0, c1 in itertools.combinations(CVSS_BASE_METRICS.keys(), 2):\n",
    "        xtab = pd.crosstab(df[c0], df[c1])\n",
    "        crosstabs[\":\".join((c0, c1))] = xtab\n",
    "        print(f\"\\n=== {c0} & {c1}\")\n",
    "        print(sm.stats.Table(xtab).resid_pearson)\n",
    "\n",
    "\n",
    "perform_independence_test(df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for statistical significance of cross-category dependence\n",
    "\n",
    "$H_{0_{\\alpha, \\beta}}$: metric $\\alpha$ and metric $\\beta$ are independent\n",
    "\n",
    "Use standard significance level $\\alpha = 0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "xtab = crosstabs[\"C:I\"]\n",
    "alpha = 0.5\n",
    "chi2stat, pvalue, dof, expected_frequency = scipy.stats.chi2_contingency(xtab)\n",
    "chi2stat, pvalue, dof, expected_frequency, pvalue <= alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "# Generate two sets of data\n",
    "data1 = np.random.normal(0, 1, 1000)\n",
    "data2 = np.random.normal(0.1, 1, 1000)\n",
    "\n",
    "# Perform a t-test\n",
    "t_stat, p_value = scipy.stats.ttest_ind(data1, data2)\n",
    "\n",
    "print(f\"t-statistic: {t_stat}\")\n",
    "print(f\"p-value: {p_value}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to understand if the results in the crosstable are statistically significant\n",
    "Then, select an approach for training (multi-class log reg?)\n",
    "\n",
    "Then, try a basic run\n",
    "\n",
    "Ok, I understand `pvalue` enough to proceed.\n",
    "Let's find the p-values of all combinations\n",
    "and put it in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalues = {}\n",
    "def calculate_p_values(df: pd.DataFrame):\n",
    "    for c0, c1 in itertools.combinations(CVSS_BASE_METRICS.keys(), 2):\n",
    "        xtab = pd.crosstab(df[c0], df[c1])\n",
    "        chi2stat, pvalue, _, _ = scipy.stats.chi2_contingency(xtab)\n",
    "        pvalues[\".\".join((c0,c1))] = pvalue\n",
    "\n",
    "calculate_p_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "pt = PrettyTable()\n",
    "pt.field_names = [\"Metric Combination\", \"Independence p-value\"]\n",
    "pt.align = \"l\"\n",
    "\n",
    "for mc, pval in pvalues.items():\n",
    "    if pval < 0.05:\n",
    "        pval = \"<0.05 (!)\"\n",
    "    pt.add_row((mc, pval))\n",
    "\n",
    "pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistically_independent = len(list(filter(lambda p: p > 0.05, pvalues.values())))\n",
    "print(\n",
    "    f\"Only {statistically_independent} combinations are statistically independent out\"\n",
    "    f\" of {len(pvalues)} combinations of cvss metrics\"\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the data and algorithm\n",
    "\n",
    "To start, I will use the UI metric because\n",
    "1. it is binomial\n",
    "1. it has a good split between the two values\n",
    "\n",
    "I will use a simple MLE logistic regression with gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"UI\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.dropna(subset=[\"UI\"])\n",
    "_, X = create_bow(df_clean[\"processed_desc\"].dropna().to_list())\n",
    "# Absorb bias into X\n",
    "X = np.insert(X, 0, 1, axis=1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.where(df[\"UI\"].dropna() == \"Required\", 1, 0)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_split = 1000\n",
    "# Transpose X such that examples are column vectors\n",
    "X_train, X_test = X[tt_split:].T, X[:tt_split].T\n",
    "Y_train, Y_test = Y[tt_split:], Y[:tt_split]\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def init_params(n_features):\n",
    "    return np.zeros(n_features)\n",
    "\n",
    "\n",
    "def num_incorrect(labels: np.ndarray, predictions: np.ndarray) -> np.ndarray:\n",
    "    c1 = (predictions > 0.5) & (labels == 1)\n",
    "    c2 = (predictions < 0.5) & (labels == 0)\n",
    "\n",
    "    return np.logical_or(c1, c2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "grad_desc_cycles = 0\n",
    "w = init_params(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(grad_desc_cycles):\n",
    "    z = w.dot(X_train)\n",
    "    predictions = sigmoid(z)\n",
    "    errors = Y_train - predictions\n",
    "    grad = errors.dot(X_train.T) / len(Y_train)\n",
    "    w = w + alpha * grad\n",
    "\n",
    "    if i % 10: continue\n",
    "    predictions_test = sigmoid(w.dot(X_test))\n",
    "    n_correct = np.count_nonzero(num_incorrect(Y_test, predictions_test))\n",
    "    print(np.around(predictions, 3), np.sum(errors), f\"({n_correct} / {len(Y_test)})\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Logistic Regression\n",
    "Now let's try a different metric, `C` (Confidentiality).\n",
    "This metric has 3 possible values, so I'll need to use\n",
    "Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "metric = \"AV\"\n",
    "\n",
    "df_clean = df.dropna(subset=[metric])\n",
    "_, X = create_bow(df_clean[\"processed_desc\"].dropna().to_list())\n",
    "# Absorb bias into X\n",
    "X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "Y = OneHotEncoder(sparse_output=False).fit_transform(\n",
    "    df[metric].dropna().to_numpy().reshape(-1, 1)\n",
    ")\n",
    "\n",
    "X.shape, Y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[metric].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_split = 1000\n",
    "# Transpose X such that examples are column vectors\n",
    "X_train, X_test = X[tt_split:].T, X[:tt_split].T\n",
    "Y_train, Y_test = Y[tt_split:], Y[:tt_split]\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z: np.ndarray):\n",
    "    return np.exp(Z) / np.sum(np.exp(Z), axis=0)\n",
    "\n",
    "\n",
    "def sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def init_params(n_features, n_categories):\n",
    "    return np.zeros((n_features, n_categories))\n",
    "\n",
    "def categorical_cross_entropy_loss(ohY: np.ndarray, A: np.ndarray) -> float:\n",
    "    assert np.all(\n",
    "        (ohY.sum(axis=1) == 1) & np.all((ohY == 0) | (ohY == 1), axis=1)\n",
    "    )  # one-hot encoding\n",
    "    EPSILON = 1e-8 # avoid math exceptions if A happens to contain 0\n",
    "    return -np.mean(np.sum(ohY * np.log(A + EPSILON), axis=1), dtype=float)\n",
    "\n",
    "def num_correct(Y: np.ndarray, A: np.ndarray) -> int:\n",
    "    return np.sum(np.argmax(Y, axis=1) == np.argmax(A, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = init_params(len(Y_train.T), len(X_train))\n",
    "alpha = 0.11\n",
    "grad_desc_cycles = 0 # disable\n",
    "\n",
    "for i in range(grad_desc_cycles):\n",
    "    Z = W.dot(X_train)\n",
    "    predictions = softmax(sigmoid(Z))\n",
    "    errors = Y_train.T - predictions\n",
    "    grad = errors.dot(X_train.T) / len(Y_train)\n",
    "    W = W + alpha * grad\n",
    "\n",
    "    if i % 10: continue\n",
    "    print(num_correct(Y_train, predictions.T), len(Y_train))\n",
    "    print(categorical_cross_entropy_loss(Y_train, predictions.T))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|metric|best result|\n",
    "|---|---|\n",
    "|C\t| 1608 |\n",
    "|I\t| 1581 |\n",
    "|AV\t| 1897 |\n",
    "|UI\t| 1895 |\n",
    "|S\t| 2211 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch\n",
    "\n",
    "I now want to explore using the same techniques,\n",
    "but with a purpose-built library like `pytorch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "metric = \"AV\"\n",
    "\n",
    "df_clean = df.dropna(subset=[metric])\n",
    "_, X = create_bow(df_clean[\"processed_desc\"].dropna().to_list())\n",
    "# Absorb bias into X\n",
    "# X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "Y = OneHotEncoder(sparse_output=False).fit_transform(\n",
    "    df[metric].dropna().to_numpy().reshape(-1, 1)\n",
    ").argmax(axis=1)\n",
    "\n",
    "X.shape, Y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# float32 and long are the usual data types\n",
    "# for features and labels in PyTorch\n",
    "X_train_torch = torch.from_numpy(X).float()\n",
    "Y_train_torch = torch.from_numpy(Y).long()\n",
    "X_train_torch.shape, Y_train_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_torch.dtype, Y_train_torch.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(X_train_torch.size(1), len(Y_train_torch.unique()))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.11)\n",
    "# Combines LogSoftMax and NLLLoss\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 0\n",
    "\n",
    "for i in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_torch)\n",
    "\n",
    "    loss = loss_fn(outputs, Y_train_torch)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 10: continue\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    correct = (predicted == Y_train_torch).sum().item()\n",
    "    print(f\"Epoch {i} Accuracy: {correct/len(Y_train_torch)} Loss: {loss.item()}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "My next task is to do a full run of training\n",
    "and then print out the results on the test data.\n",
    "\n",
    "I want to see a table like this:\n",
    "\n",
    "- av: (45 / 100)\n",
    "- i: (77 / 100)\n",
    "- ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: prepare labels and features\n",
    "\n",
    "Crucial: Y can be fully split and preprocessed before training; X cannot, since\n",
    "the training data alone must be the basis for vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# pick any metric to remove NaNs\n",
    "df_clean = df.dropna(subset=[\"AV\"]).copy()\n",
    "\n",
    "for metric in CVSS_BASE_METRICS.keys():\n",
    "    encoder = LabelEncoder()\n",
    "    df_clean[metric] = encoder.fit_transform(df[metric].dropna())\n",
    "\n",
    "Y_np = df_clean[list(CVSS_BASE_METRICS.keys())].values\n",
    "Y = torch.from_numpy(Y_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data and create Y matrices\n",
    "train_split = 0.8\n",
    "i = int(0.8 * len(X))\n",
    "X_train_raw, X_test_raw = df_clean[\"processed_desc\"][:i], df_clean[\"processed_desc\"][i:]\n",
    "Y_train, Y_test = Y[:i], Y[i:]\n",
    "\n",
    "# compute X_train_np just so we can examine the shape;\n",
    "# the actual X_train will be constructed just before training\n",
    "bow_vec, X_train_np = create_bow(X_train_raw.to_list())\n",
    "X_train_np.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cve_engine.engine import CVEEngineModel\n",
    "\n",
    "cvem = CVEEngineModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = True\n",
    "\n",
    "if load:\n",
    "    cvem.load_latest_models()\n",
    "    cvem.display_parameters()\n",
    "else:\n",
    "    cvem.new_model(bow_vec)\n",
    "    cvem.display_parameters()\n",
    "    cvem.train_all(X_train_raw.to_numpy(), Y_train)\n",
    "    cvem.save_models_full()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, cs = cvem.predict(X_test_raw.to_numpy())\n",
    "pred, cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pct correct\n",
    "np.mean(Y_test.numpy() == pred, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average confidence scores\n",
    "np.mean(cs, axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some thoughts\n",
    "\n",
    "- Want to allow for a \"human-in-the-loop\" feedback mechanism\n",
    "- Using \"active learning\", the model asks for feedback when it is less confident\n",
    "- Probably what I would use is closer to \"online learning\", where I can simply\n",
    "  scrape the data after the fact and retrain.\n",
    "- might be interesting to see what words are the strongest predictors for certain metrics / categories."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Analyzing the CVE index file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "files1 = set(os.listdir(\"../data/cve/2021\"))\n",
    "files2 = set(os.listdir(\"../data/cve/2022\"))\n",
    "files3 = set(os.listdir(\"../data/cve/2023\"))\n",
    "\n",
    "files1 & files2, files2 & files3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_index():\n",
    "    with open(\"../data/cve/cve_index.json\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "cve_index = load_index()\n",
    "len(cve_index[\"cve_refs\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for comm in files1 & files2:\n",
    "    comm = comm.removesuffix(\".json\")\n",
    "    print(cve_index[\"cve_refs\"].get(comm))\n",
    "\n",
    "print(\"-----\")\n",
    "\n",
    "for comm in files2 & files3:\n",
    "    comm = comm.removesuffix(\".json\")\n",
    "    print(cve_index[\"cve_refs\"].get(comm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_cve_json_files(cve_id: str):\n",
    "    filename = f\"{cve_id}.json\"\n",
    "    for subdir in (\"2021\", \"2022\", \"2023\"):\n",
    "        path = os.path.join(\"../data/cve\", subdir, filename)\n",
    "        if os.path.isfile(path):\n",
    "            return path\n",
    "    return None\n",
    "\n",
    "\n",
    "def search_cve_index_missing_cves():\n",
    "    c = []\n",
    "    for cve_id in df[\"CVE_ID\"]:\n",
    "        if cve_id not in cve_index[\"cve_refs\"]:\n",
    "            print(\"index: \", cve_id)\n",
    "        if not search_cve_json_files(cve_id):\n",
    "            if \"2023\" not in cve_id:\n",
    "                continue\n",
    "            c.append(cve_id)\n",
    "            print(\"local: \", cve_id)\n",
    "    print(len(c))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many contributions have cvss vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cves():\n",
    "    \"\"\"Loads all cve data, indexed by cve_id\"\"\"\n",
    "    cves = {}\n",
    "    for subdir in (\"2017\", \"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\"):\n",
    "        path = os.path.join(\"../data/cve\", subdir)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path, file)) as f:\n",
    "                cves[file.removesuffix(\".json\")] = json.load(f)\n",
    "    return cves\n",
    "\n",
    "cves = load_cves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_source_values = [cve[\"desc_source\"] for cve in cves.values()]\n",
    "source_data_contributions = list(itertools.chain(*(cve[\"source_data\"] for cve in cves.values())))\n",
    "source_data_source_names = [sdc[\"source_name\"] for sdc in source_data_contributions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(source_data_source_names, edgecolor=\"black\")\n",
    "plt.xlabel(\"source_name\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of source_data.source_name values\")\n",
    "plt.savefig(\"../source_providers.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_cvss_scores():\n",
    "    scores = []\n",
    "    for cve_id, cve in cves.items():\n",
    "        for sd in cve[\"source_data\"]:\n",
    "            if \"scores\" not in sd: continue\n",
    "            scores.extend(sd[\"scores\"])\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame(extract_all_cvss_scores())\n",
    "df_scores[\"source_name\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores[\"vector_clean\"] = df_scores[\"vector\"].apply(clean_cvss_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.pivot(index=None, columns=\"source_name\", values=\"vector_clean\").apply(\n",
    "    lambda col: col.dropna().reset_index(drop=True)\n",
    ").to_csv(\"../all_vectors.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of disagreement\n",
    "Output a csv with each row being a time where AL and REDHAT differ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "entity_name = os.environ[\"ENTITY_NAME\"]\n",
    "\n",
    "df_rh = df_scores[df_scores[\"source_name\"] == \"REDHAT\"]\n",
    "df_en = df_scores[df_scores[\"source_name\"] == entity_name]\n",
    "df_rh = df_rh.rename(columns={\"vector_clean\": \"vector_clean_rh\", \"vector\": \"vector_rh\"})\n",
    "df_en = df_en.rename(columns={\"vector_clean\": \"vector_clean_en\", \"vector\": \"vector_en\"})\n",
    "df_merged = pd.merge(df_rh, df_en, on=\"cve_id\")\n",
    "\n",
    "df_res = df_merged[df_merged[\"vector_clean_rh\"] != df_merged[\"vector_clean_en\"]]\n",
    "df_res_agree = df_merged[df_merged[\"vector_clean_rh\"] == df_merged[\"vector_clean_en\"]]\n",
    "df_res = df_res[\n",
    "    [\"cve_id\", \"vector_rh\", \"vector_en\", \"vector_clean_rh\", \"vector_clean_en\"]\n",
    "]\n",
    "df_res_agree = df_res_agree[\n",
    "    [\"cve_id\", \"vector_rh\", \"vector_en\", \"vector_clean_rh\", \"vector_clean_en\"]\n",
    "]\n",
    "df_res.to_csv(\"../redhat_name_disagreement.csv\")\n",
    "df_res_agree.to_csv(\"../redhat_name_agreement.csv\")\n",
    "len(df_rh), len(df_en), len(df_merged), len(df_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import seaborn as sns\n",
    "\n",
    "groups = df_scores.groupby(\"cve_id\")\n",
    "pairwise_agreements = collections.defaultdict(int)\n",
    "\n",
    "for _, group in groups:\n",
    "    unique_source_names = group[\"source_name\"].unique()\n",
    "\n",
    "    for src0, src1 in itertools.combinations(unique_source_names, 2):\n",
    "        if (\n",
    "            group[group[\"source_name\"] == src0][\"vector_clean\"].iloc[0]\n",
    "            == group[group[\"source_name\"] == src1][\"vector_clean\"].iloc[0]\n",
    "        ):\n",
    "            pairwise_agreements[(src0, src1)] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df_scores.sort_values(\"cve_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pairs = pd.DataFrame.from_dict(pairwise_agreements, orient='index', columns=['agreements']).reset_index()\n",
    "df_pairs[\"agreement_rate\"] = df_pairs[\"agreements\"] / len(groups)\n",
    "df_pairs.to_csv(\"../agreement_rates.csv\")\n",
    "df_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Training attempt with max vector contributions\n",
    "\n",
    "Here, I am throwing caution to the wind and using the maximum amount of\n",
    "data I have available to me (at least from 2021, 2022, and 0.5 * 2023).\n",
    "\n",
    "#### Step 1: assemble the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can take a few seconds\n",
    "cves = load_cves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note to self: each index in `cves` has an array called `source_data`;\n",
    "this is my true raw data.  If a `source_data` entry has all of the following, I will include it.\n",
    "1. `cve_id`\n",
    "1. `description`\n",
    "1. `scores.[].vector`\n",
    "\n",
    "The parent description should be copied to each of the vectors in `scores`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_training_set(cves: dict):\n",
    "    \"\"\"\n",
    "    Scan through all CVEs for cve.source_data elements.\n",
    "    For each element, couple the cve.source_data.elem.description\n",
    "    with each cve.source_data.elem.score.\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    for cve_data in cves.values():\n",
    "        for sd in cve_data[\"source_data\"]:\n",
    "            if \"scores\" not in sd: continue\n",
    "            examples.extend(\n",
    "                [\n",
    "                    {\"description\": sd[\"description\"]} | score\n",
    "                    for score in sd[\"scores\"]\n",
    "                ]\n",
    "            )\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = pd.DataFrame(construct_training_set(cves))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"cve_engine.data_processing\").setLevel(logging.INFO)\n",
    "# some repeated code here\n",
    "from cve_engine.cvss_data import CVSS_BASE_METRICS\n",
    "from cve_engine.data_processing import (\n",
    "    clean_cvss_vector,\n",
    "    desc_preprocess,\n",
    "    vec_parse_metric,\n",
    "    create_bow,\n",
    ")\n",
    "\n",
    "def extract_cvss_vector_components(df: pd.DataFrame, vector: pd.Series):\n",
    "    for metric in CVSS_BASE_METRICS:\n",
    "        df[metric] = vector.dropna().apply(lambda v: vec_parse_metric(v, metric))\n",
    "    return df\n",
    "\n",
    "df_x[\"vector_clean\"] = df_x[\"vector\"].apply(clean_cvss_vector)\n",
    "df_x[\"processed_desc\"] = df_x[\"description\"].apply(desc_preprocess)\n",
    "df_x = extract_cvss_vector_components(df_x, df_x[\"vector_clean\"])\n",
    "\n",
    "df_x.to_csv(\"../df_x.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only this compact is version is used going forward\n",
    "df_x_clean = df_x.dropna(subset=[\"vector_clean\"]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "for metric in CVSS_BASE_METRICS.keys():\n",
    "    encoder = LabelEncoder()\n",
    "    df_x_clean[metric + \"_Y\"] = encoder.fit_transform(df_x_clean[metric])\n",
    "\n",
    "Y_np = df_x_clean[[metric + \"_Y\" for metric in CVSS_BASE_METRICS.keys()]].values\n",
    "Y = torch.from_numpy(Y_np)\n",
    "\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data and create Y matrices\n",
    "train_split = 0.8\n",
    "i = int(0.8 * len(Y))\n",
    "X_train_raw, X_test_raw = df_x_clean[\"processed_desc\"][:i], df_x_clean[\"processed_desc\"][i:]\n",
    "Y_train, Y_test = Y[:i], Y[i:]\n",
    "\n",
    "# compute X_train_np just so we can examine the shape;\n",
    "# the actual X_train will be constructed just before training\n",
    "bow_vec, X_train_np = create_bow(X_train_raw.to_list())\n",
    "X_train_np.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cve_engine.engine import CVEEngineModel\n",
    "\n",
    "cvem = CVEEngineModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvem.new_model(bow_vec)\n",
    "cvem.display_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvem.train_all(X_train_raw.to_numpy(), Y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Appendix\n",
    "\n",
    "Awesome extra stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = bow_vec.get_feature_names_out()\n",
    "word_counts = X.sum(axis=0)\n",
    "counts_and_words = sorted(zip(word_counts, feature_names), reverse=True)\n",
    "top_20 = counts_and_words[:20]\n",
    "\n",
    "counts, words = zip(*top_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(words, counts)\n",
    "plt.xlabel(\"Keyword\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Top 20 CVE Description Keyword Frequencies\")\n",
    "plt.xticks(rotation=60)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../top_20_words.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def calculate_p_values(df: pd.DataFrame):\n",
    "    pvalues = pd.DataFrame(\n",
    "        index=pd.Index(CVSS_BASE_METRICS.keys()), columns=list(CVSS_BASE_METRICS.keys())\n",
    "    )\n",
    "\n",
    "    for c0, c1 in itertools.combinations(CVSS_BASE_METRICS.keys(), 2):\n",
    "        xtab = pd.crosstab(df[c0], df[c1])\n",
    "        chi2stat, pvalue, _, _ = scipy.stats.chi2_contingency(xtab)\n",
    "        pvalues.loc[c0, c1] = pvalue\n",
    "        pvalues.loc[c1, c0] = pvalue\n",
    "\n",
    "    np.fill_diagonal(pvalues.values, 0)\n",
    "\n",
    "    return pvalues.apply(pd.to_numeric)\n",
    "\n",
    "\n",
    "pvalues = calculate_p_values(df)\n",
    "pvalues.columns = list(metric.name for metric in CVSS_BASE_METRICS.values())\n",
    "pvalues.index = pd.Index(metric.name for metric in CVSS_BASE_METRICS.values())\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(pvalues, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"CVSS Vector Metric Correlation Heatmap\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../heatmap.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_words = df_clean[\"DESCRIPTION\"].apply(str.split).apply(len).mean()\n",
    "avg_processed_words = df_clean[\"processed_desc\"].apply(str.split).apply(len).mean()\n",
    "avg_words, avg_processed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "stmts = {\n",
    "    \"48.4\": \"Mean number of words in a CVE description\",\n",
    "    \"36.3\": \"Mean number of words in a CVE description after processing\",\n",
    "}\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    nrows=len(stmts), figsize=(6, len(stmts) * 2), tight_layout=True\n",
    ")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "for ax, (key, value) in zip(axs, stmts.items()):\n",
    "    ax.text(0.1, 0.6, key, fontsize=24, va=\"center\", weight=\"bold\", color=\"blue\")\n",
    "    ax.text(0.1, 0.3, value, fontsize=12, va=\"center\", color=\"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../other_stats.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "df_melted = df[list(CVSS_BASE_METRICS.keys())].melt(\n",
    "    var_name=\"metric_key\", value_name=\"category\"\n",
    ")\n",
    "\n",
    "df_grouped = df_melted.groupby([\"metric_key\", \"category\"]).size().unstack()\n",
    "df_grouped.index = df_grouped.index.map(\n",
    "    {k: v.name for k, v in CVSS_BASE_METRICS.items()}\n",
    ")\n",
    "\n",
    "ax = df_grouped.plot(kind=\"bar\", stacked=True)\n",
    "plt.ylabel(\"Category counts\")\n",
    "plt.xlabel(\"CVSS Metric\")\n",
    "plt.title(\"CVSS Metric Category Values\")\n",
    "\n",
    "for i, (index, row) in enumerate(df_grouped.iterrows()):\n",
    "    cumulative_size = 0\n",
    "\n",
    "    for col in df_grouped.columns:\n",
    "        value = row[str(col)]\n",
    "\n",
    "        if np.isnan(value):\n",
    "            continue\n",
    "\n",
    "        x_position = i\n",
    "        y_position = cumulative_size + (value / 2)\n",
    "\n",
    "        ax.text(x_position, y_position, str(col), ha=\"center\", va=\"center\")\n",
    "\n",
    "        cumulative_size += value\n",
    "\n",
    "ax.legend().remove()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../stacks.png\", dpi=500)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "pred, _ = cvem.predict(X_test_raw.to_numpy())\n",
    "\n",
    "confusion_matrices = {}\n",
    "\n",
    "for i, metric in enumerate(CVSS_BASE_METRICS):\n",
    "    pred_for_model = pred[:, i]\n",
    "    conf_matrix = confusion_matrix(Y_test[:, i], pred_for_model)\n",
    "    confusion_matrices[metric] = conf_matrix\n",
    "\n",
    "for metric, matrix in confusion_matrices.items():\n",
    "    print(f\"Confusion matrix for {metric}:\")\n",
    "    print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, (metric, matrix) in zip(axes, confusion_matrices.items()):\n",
    "    class_names = CVSS_BASE_METRICS[metric].categories\n",
    "    sns.heatmap(\n",
    "        matrix,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names,\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(CVSS_BASE_METRICS[metric].name)\n",
    "    ax.set_ylabel(\"Actual\")\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../confusion_matrices.png\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
