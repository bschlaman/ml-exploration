{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CVE Analysis Engine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cvss\n",
    "import cvss.exceptions\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"../data/cve/cves.json\")\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calc_cvss(v: str) -> float:\n",
    "    try:\n",
    "        return cvss.CVSS3(v).scores()[0]\n",
    "    except cvss.exceptions.CVSS3MalformedError:\n",
    "        return -1.0\n",
    "\n",
    "\n",
    "df[\"scores\"] = df[\"XYZ_CVSS_VECTOR\"].dropna().apply(_calc_cvss)\n",
    "df[\"bad\"] = df[\"XYZ_CVSS_SCORE\"].notna() & (df[\"XYZ_CVSS_SCORE\"] != df[\"scores\"])\n",
    "df[[\"XYZ_CVSS_SCORE\", \"scores\", \"bad\"]].to_csv(\"../output.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 1: Logistic Regression\n",
    "\n",
    "Logistic Regression is often referred to as the _discriminative_\n",
    "counterpart of Naive Bayes.\n",
    "\n",
    "Model $P(y | \\mathbf{x}_i)$ and assume it takes exactly the form\n",
    "\n",
    "$$\n",
    "    P(y | \\mathbf{x}_i) = \\frac{1}{1 + e^{-y(\\mathbf{w}^T\\mathbf{x}_i + b)}}\n",
    "$$\n",
    "\n",
    "while making few assumptions about $P(\\mathbf{x}_i | y)$.\n",
    "Ultimately it doesn't matter, because we estimate $\\mathbf{w}$ and $b$\n",
    "directly with MLE or MAP to maximize the conditional likelihood of\n",
    "\n",
    "$$\n",
    "    \\prod_i P(y_i | \\mathbf{x}_i; \\mathbf{w}, b)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLE\n",
    "\n",
    "Choose parameters that maximize the conditional likelihood.\n",
    "The conditional data likelihood $P(\\mathbf{y} | X, \\mathbf{w})$\n",
    "is the probability of the observed values $\\mathbf{y} \\in \\mathbb{R}^n$\n",
    "in the training data conditioned on the feature values $\\mathbf{x}_i$.\n",
    "Note that $X = [\\mathbf{x}_1,\\dots,\\mathbf{x}_n] \\in \\mathbb{R}^{d \\times n}$.\n",
    "We choose the parameters that maximize this function, and we assume that\n",
    "the $y_i$ are independent given the input features $\\mathbf{x}_i$ and $\\mathbf{w}$.\n",
    "\n",
    "> In my view, for CVE vectors, this assumption is perfectly valid to make\n",
    "\n",
    "$$\n",
    "    P(\\mathbf{y} | X, \\mathbf{w}) = \\prod_{i=1}^{n} P(y_i | \\mathbf{x}_i, \\mathbf{w}) \\\\\n",
    "    \\hat{\\mathbf{w}}_{\\text{MLE}}\n",
    "    = \\underset{\\mathbf{w}}{\\arg\\max}\n",
    "    - \\sum_{i=1}^{n}\\log(1 + e^{-y_i\\mathbf{w}^T\\mathbf{x}_i}) \\\\\n",
    "    = \\underset{\\mathbf{w}}{\\arg\\min} \\sum_{i=1}^{n}\\log(1 + e^{-y_i\\mathbf{w}^T\\mathbf{x}_i})\n",
    "$$\n",
    "\n",
    "Use gradient descent on the _negative log likelihood_.\n",
    "\n",
    "$$\n",
    "    \\ell(\\mathbf{w}) = \\sum_{i=1}^{n}\\log(1 + e^{-y_i\\mathbf{w}^T\\mathbf{x}_i})\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing\n",
    "\n",
    "1. lowercase all text\n",
    "1. remove punctuation\n",
    "1. tokenize\n",
    "1. remove stop words\n",
    "1. lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "def desc_preprocess(d: str):\n",
    "    # setup\n",
    "    stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    # lowercase\n",
    "    d = d.lower()\n",
    "    # remove punctuation\n",
    "    d = d.translate(str.maketrans(string.punctuation, \" \"*len(string.punctuation)))\n",
    "    # tokenize\n",
    "    tokens = d.split()\n",
    "    # remove stop words\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DESCRIPTION\"].apply(desc_preprocess).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def create_bow(descs: pd.Series) -> np.ndarray:\n",
    "    return CountVectorizer().fit_transform(descs).toarray()\n",
    "\n",
    "X = create_bow(df[\"DESCRIPTION\"])\n",
    "\n",
    "len(X), len(X.T)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem statement\n",
    "\n",
    "There are two sides to my problem:\n",
    "\n",
    "1. **Given input descriptions, predict the cvss vector.**\n",
    "   This is a multi-label, multi-class classification problem.\n",
    "   Some potential strategies are defined below.\n",
    "1. **Given input descriptions, suggest a cvss score directly.**\n",
    "   This is probably a regression problem, although it can\n",
    "   be converted into a classification problem with buckets\n",
    "   score buckets of some discrete size.\n",
    "\n",
    "- Independent labels: train a separate classifier for each label, probably using softmax regression\n",
    "- Dependent labels: classifier chains - input to a classifier includes output from another\n",
    "- Dependent labels: label powerset - transform problem into a multi-class problem\n",
    "  with one multi-class classifier is trained on all unique label combinations\n",
    "  found in the training data.  Deals efficiently with label correlations.\n",
    "\n",
    "I need to make a decision regarding the independence assumption of my labels.\n",
    "I find it intellectually interesting to explore the correlation statistics\n",
    "between the category + label combinations.  Two methods for establishing\n",
    "correlation between categories is\n",
    "- Chi-square test of independence\n",
    "- Cramer's V\n",
    "\n",
    "#### Next steps\n",
    "\n",
    "- Look at documentation to make sure I've got my problem statements right.\n",
    "  Does vector suggestion deliver value?\n",
    "- Perform a *Cramer's V* analysis on training examples\n",
    "- Based on the output of this, decide on ml strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import Union\n",
    "\n",
    "metrics = {\n",
    "    \"AV\": \"Attack Vector\",\n",
    "    \"AC\": \"Attack Complexity\",\n",
    "    \"PR\": \"Privileges Required\",\n",
    "    \"UI\": \"User Interaction\",\n",
    "    \"S\": \"Scope\",\n",
    "    \"C\": \"Confidentiality\",\n",
    "    \"I\": \"Integrity\",\n",
    "    \"A\": \"Availability\",\n",
    "}\n",
    "\n",
    "\n",
    "def _vec_parse(vec: str, metric: str):\n",
    "    try:\n",
    "        return cvss.CVSS3(vec).get_value_description(metric)\n",
    "    except (cvss.exceptions.CVSS3MalformedError, AttributeError):\n",
    "        print(vec)\n",
    "        return \"XXXX\"\n",
    "\n",
    "def clean_cvss_vector(vec: Union[str, float]) -> str:\n",
    "    if pd.isna(vec): return vec\n",
    "    try:\n",
    "        return cvss.CVSS3(vec).clean_vector()\n",
    "    except cvss.exceptions.CVSS3MalformedError:\n",
    "        pass\n",
    "\n",
    "    # fix common problems\n",
    "    vec = vec.upper()\n",
    "    vec = vec.replace(\" \", \"\")\n",
    "    vec = vec.rstrip(\"/\")\n",
    "    try:\n",
    "        vec = \"CVSS:3.1/\" + vec[vec.index(\"AV:\"):]\n",
    "    except ValueError:\n",
    "        pass\n",
    "    # vec = vec.removeprefix(\"VECTOR:\")\n",
    "    # if vec.startswith(\"AV\"): vec = \"CVSS:3.1/\" + vec\n",
    "    # if vec.startswith(\"/AV\"): vec = \"CVSS:3.1\" + vec\n",
    "\n",
    "    # try again\n",
    "    try:\n",
    "        return cvss.CVSS3(vec).clean_vector()\n",
    "    except cvss.exceptions.CVSS3MalformedError:\n",
    "        return vec\n",
    "    \n",
    "\n",
    "def extract_cvss_vector_components(df: pd.DataFrame, vector: pd.Series):\n",
    "    for metric in metrics.keys():\n",
    "        df[metric] = vector.apply(lambda v: _vec_parse(v, metric))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"vector\"] = df.XYZ_CVSS_VECTOR.apply(clean_cvss_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extract_cvss_vector_components(df, df[\"vector\"])\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = 'CVSS:3.0/S:C/C:H/I:H/A:N/AV:P/AC:H/PR:H/UI:R/E:H/RL:O/RC:R/CR:H/IR:X/AR:X/MAC:H/MPR:X/MUI:X/MC:L/MA:X'\n",
    "c = cvss.CVSS3(vector)\n",
    "\n",
    "c.get_value_description(\"S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "tabs = pd.crosstab(*[df[metric] for metric in metrics.keys()])\n",
    "table = statsmodels.stats.Table(tabs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
