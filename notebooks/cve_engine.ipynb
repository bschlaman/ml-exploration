{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CVE Analysis Engine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cvss\n",
    "import cvss.exceptions\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"../data/cve/cves.json\")\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calc_cvss(v: str) -> float:\n",
    "    try:\n",
    "        return cvss.CVSS3(v).scores()[0]\n",
    "    except cvss.exceptions.CVSS3MalformedError:\n",
    "        return -1.0\n",
    "\n",
    "df['scores'] = df[\"cvss_VECTOR\"].dropna().apply(_calc_cvss)\n",
    "df['bad'] = df[\"cvss_SCORE\"].notna() & (df[\"cvss_SCORE\"] != df['scores'])\n",
    "df[[\"cvss_SCORE\", \"scores\", 'bad']].to_csv(\"../output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 1: Logistic Regression\n",
    "\n",
    "Logistic Regression is often referred to as the _discriminative_\n",
    "counterpart of Naive Bayes.\n",
    "\n",
    "Model $P(y | \\mathbf{x}_i)$ and assume it takes exactly the form\n",
    "\n",
    "$$\n",
    "    P(y | \\mathbf{x}_i) = \\frac{1}{1 + e^{-y(\\mathbf{w}^T\\mathbf{x}_i + b)}}\n",
    "$$\n",
    "\n",
    "while making few assumptions about $P(\\mathbf{x}_i | y)$.\n",
    "Ultimately it doesn't matter, because we estimate $\\mathbf{w}$ and $b$\n",
    "directly with MLE or MAP to maximize the conditional likelihood of\n",
    "\n",
    "$$\n",
    "    \\prod_i P(y_i | \\mathbf{x}_i; \\mathbf{w}, b)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLE\n",
    "\n",
    "Choose parameters that maximize the conditional likelihood.\n",
    "The conditional data likelihood $P(\\mathbf{y} | X, \\mathbf{w})$\n",
    "is the probability of the observed values $\\mathbf{y} \\in \\mathbb{R}^n$\n",
    "in the training data conditioned on the feature values $\\mathbf{x}_i$.\n",
    "Note that $X = [\\mathbf{x}_1,\\dots,\\mathbf{x}_n] \\in \\mathbb{R}^{d \\times n}$.\n",
    "We choose the parameters that maximize this function, and we assume that\n",
    "the $y_i$ are independent given the input features $\\mathbf{x}_i$ and $\\mathbf{w}$.\n",
    "\n",
    "> In my view, for CVE vectors, this assumption is perfectly valid to make\n",
    "\n",
    "$$\n",
    "    P(\\mathbf{y} | X, \\mathbf{w}) = \\prod_{i=1}^{n} P(y_i | \\mathbf{x}_i, \\mathbf{w}) \\\\\n",
    "    \\hat{\\mathbf{w}}_{\\text{MLE}}\n",
    "    = \\underset{\\mathbf{w}}{\\arg\\max}\n",
    "    - \\sum_{i=1}^{n}\\log(1 + e^{-y_i\\mathbf{w}^T\\mathbf{x}_i}) \\\\\n",
    "    = \\underset{\\mathbf{w}}{\\arg\\min} \\sum_{i=1}^{n}\\log(1 + e^{-y_i\\mathbf{w}^T\\mathbf{x}_i})\n",
    "$$\n",
    "\n",
    "Use gradient descent on the _negative log likelihood_.\n",
    "\n",
    "$$\n",
    "    \\ell(\\mathbf{w}) = \\sum_{i=1}^{n}\\log(1 + e^{-y_i\\mathbf{w}^T\\mathbf{x}_i})\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing\n",
    "\n",
    "1. lowercase all text\n",
    "1. remove punctuation\n",
    "1. tokenize\n",
    "1. remove stop words\n",
    "1. lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "def desc_preprocess(d: str):\n",
    "    # setup\n",
    "    stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    # lowercase\n",
    "    d = d.lower()\n",
    "    # remove punctuation\n",
    "    d = d.translate(str.maketrans(string.punctuation, \" \"*len(string.punctuation)))\n",
    "    # tokenize\n",
    "    tokens = d.split()\n",
    "    # remove stop words\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"description\"].apply(desc_preprocess).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def create_bow(descs: pd.Series) -> np.ndarray:\n",
    "    return CountVectorizer().fit_transform(descs).toarray()\n",
    "\n",
    "X = create_bow(df[\"description\"])\n",
    "\n",
    "len(X), len(X.T)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
