{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CVE Analysis Engine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cvss\n",
    "import cvss.exceptions\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True, raise_on_error=True)\n",
    "nltk.download(\"wordnet\", quiet=True, raise_on_error=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"../data/cve/cves.json\")\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calc_cvss_score(v: str) -> float:\n",
    "    try:\n",
    "        return cvss.CVSS3(v).scores()[0]\n",
    "    except cvss.exceptions.CVSS3MalformedError:\n",
    "        return -1.0\n",
    "\n",
    "\n",
    "df[\"parsed_scores\"] = df[\"XYZ_CVSS_VECTOR\"].dropna().apply(_calc_cvss_score)\n",
    "df[\"failed_to_parse\"] = df[\"XYZ_CVSS_SCORE\"].notna() & (df[\"XYZ_CVSS_SCORE\"] != df[\"parsed_scores\"])\n",
    "# df[[\"XYZ_CVSS_SCORE\", \"parsed_scores\", \"failed_to_parse\"]].to_csv(\"../unparseable_vectors.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 1: Logistic Regression\n",
    "\n",
    "Logistic Regression is often referred to as the _discriminative_\n",
    "counterpart of Naive Bayes.\n",
    "\n",
    "Model $P(y | \\mathbf{x}_i)$ and assume it takes exactly the form\n",
    "\n",
    "$$\n",
    "    P(y | \\mathbf{x}_i) = \\frac{1}{1 + e^{-y(\\mathbf{w}^T\\mathbf{x}_i + b)}}\n",
    "$$\n",
    "\n",
    "while making few assumptions about $P(\\mathbf{x}_i | y)$.\n",
    "Ultimately it doesn't matter, because we estimate $\\mathbf{w}$ and $b$\n",
    "directly with MLE or MAP to maximize the conditional likelihood of\n",
    "\n",
    "$$\n",
    "    \\prod_i P(y_i | \\mathbf{x}_i; \\mathbf{w}, b)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLE\n",
    "\n",
    "Choose parameters that maximize the conditional likelihood.\n",
    "The conditional data likelihood $P(\\mathbf{y} | X, \\mathbf{w})$\n",
    "is the probability of the observed values $\\mathbf{y} \\in \\mathbb{R}^n$\n",
    "in the training data conditioned on the feature values $\\mathbf{x}_i$.\n",
    "Note that $X = [\\mathbf{x}_1,\\dots,\\mathbf{x}_n] \\in \\mathbb{R}^{d \\times n}$.\n",
    "We choose the parameters that maximize this function, and we assume that\n",
    "the $y_i$ are independent given the input features $\\mathbf{x}_i$ and $\\mathbf{w}$.\n",
    "\n",
    "> In my view, for CVE vectors, this assumption is perfectly valid to make\n",
    "\n",
    "$$\n",
    "    P(\\mathbf{y} | X, \\mathbf{w}) = \\prod_{i=1}^{n} P(y_i | \\mathbf{x}_i, \\mathbf{w}) \\\\\n",
    "    \\hat{\\mathbf{w}}_{\\text{MLE}}\n",
    "    = \\underset{\\mathbf{w}}{\\arg\\max}\n",
    "    - \\sum_{i=1}^{n}\\log(1 + e^{-y_i\\mathbf{w}^T\\mathbf{x}_i}) \\\\\n",
    "    = \\underset{\\mathbf{w}}{\\arg\\min} \\sum_{i=1}^{n}\\log(1 + e^{-y_i\\mathbf{w}^T\\mathbf{x}_i})\n",
    "$$\n",
    "\n",
    "Use gradient descent on the _negative log likelihood_.\n",
    "\n",
    "$$\n",
    "    \\ell(\\mathbf{w}) = \\sum_{i=1}^{n}\\log(1 + e^{-y_i\\mathbf{w}^T\\mathbf{x}_i})\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing\n",
    "\n",
    "1. lowercase all text\n",
    "1. remove punctuation\n",
    "1. tokenize\n",
    "1. remove stop words\n",
    "1. lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "def desc_preprocess(d: str):\n",
    "    # setup\n",
    "    stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    # lowercase\n",
    "    d = d.lower()\n",
    "    # remove punctuation\n",
    "    d = d.translate(str.maketrans(string.punctuation, \" \"*len(string.punctuation)))\n",
    "    # tokenize\n",
    "    tokens = d.split()\n",
    "    # remove stop words\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"processed_desc\"] = df[\"DESCRIPTION\"].apply(desc_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def create_bow(descs: pd.Series) -> np.ndarray:\n",
    "    return CountVectorizer().fit_transform(descs).toarray()\n",
    "\n",
    "X = create_bow(df[\"processed_desc\"])\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem statement\n",
    "\n",
    "There are two sides to my problem:\n",
    "\n",
    "1. **Given input descriptions, predict the cvss vector.**\n",
    "   This is a multi-label, multi-class classification problem.\n",
    "   Some potential strategies are defined below.\n",
    "1. **Given input descriptions, suggest a cvss score directly.**\n",
    "   This is probably a regression problem, although it can\n",
    "   be converted into a classification problem with buckets\n",
    "   score buckets of some discrete size.\n",
    "\n",
    "- Independent labels: train a separate classifier for each label, probably using softmax regression\n",
    "- Dependent labels: classifier chains - input to a classifier includes output from another\n",
    "- Dependent labels: label powerset - transform problem into a multi-class problem\n",
    "  with one multi-class classifier is trained on all unique label combinations\n",
    "  found in the training data.  Deals efficiently with label correlations.\n",
    "\n",
    "I need to make a decision regarding the independence assumption of my labels.\n",
    "I find it intellectually interesting to explore the correlation statistics\n",
    "between the category + label combinations.  Two methods for establishing\n",
    "correlation between categories is\n",
    "- Chi-square test of independence\n",
    "- Cramer's V\n",
    "\n",
    "#### Next steps\n",
    "\n",
    "- Look at documentation to make sure I've got my problem statements right.\n",
    "  Does vector suggestion deliver value?\n",
    "- Perform a *Cramer's V* analysis on training examples\n",
    "- Based on the output of this, decide on ml strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "metrics = {\n",
    "    \"AV\": \"Attack Vector\",\n",
    "    \"AC\": \"Attack Complexity\",\n",
    "    \"PR\": \"Privileges Required\",\n",
    "    \"UI\": \"User Interaction\",\n",
    "    \"S\": \"Scope\",\n",
    "    \"C\": \"Confidentiality\",\n",
    "    \"I\": \"Integrity\",\n",
    "    \"A\": \"Availability\",\n",
    "}\n",
    "\n",
    "\n",
    "def _vec_parse(vec: str, metric: str):\n",
    "    return cvss.CVSS3(vec).get_value_description(metric)\n",
    "\n",
    "def clean_cvss_vector(vec: Union[str, float]) -> Union[str, float]:\n",
    "    if pd.isna(vec): return np.nan\n",
    "    try:\n",
    "        return cvss.CVSS3(vec).clean_vector()\n",
    "    except cvss.exceptions.CVSS3MalformedError:\n",
    "        pass\n",
    "\n",
    "    # fix common problems\n",
    "    assert type(vec) is str\n",
    "    vec = vec.upper()\n",
    "    vec = vec.rstrip(\".\")\n",
    "    vec = vec.replace(\" \", \"\")\n",
    "    vec = vec.rstrip(\"/\")\n",
    "    try:\n",
    "        vec = \"CVSS:3.1/\" + vec[vec.index(\"AV:\"):]\n",
    "    except ValueError:\n",
    "        pass\n",
    "    # vec = vec.removeprefix(\"VECTOR:\")\n",
    "    # if vec.startswith(\"AV\"): vec = \"CVSS:3.1/\" + vec\n",
    "    # if vec.startswith(\"/AV\"): vec = \"CVSS:3.1\" + vec\n",
    "\n",
    "    # try again\n",
    "    try:\n",
    "        return cvss.CVSS3(vec).clean_vector()\n",
    "    except cvss.exceptions.CVSS3MalformedError:\n",
    "        return np.nan\n",
    "    \n",
    "\n",
    "def extract_cvss_vector_components(df: pd.DataFrame, vector: pd.Series):\n",
    "    for metric in metrics.keys():\n",
    "        df[metric] = vector.dropna().apply(lambda v: _vec_parse(v, metric))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"vector\"] = df.XYZ_CVSS_VECTOR.apply(clean_cvss_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extract_cvss_vector_components(df, df[\"vector\"])\n",
    "print(f\"rows with valid vectors: {df['vector'].count()}\")\n",
    "\n",
    "df.to_csv(\"../df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"processed_desc\", \"AV\"]].to_csv(\"../for_autogluon.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson's $\\Chi^2$ test\n",
    "\n",
    "- [Pearson's chi-squared test - Wikipedia](https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test)\n",
    "\n",
    "Based on the below analysis, a couple of the highest correlations:\n",
    "\n",
    "- AV:L & PR:L highly correlated\n",
    "- AV:N & PR:L highly negatively correlated\n",
    "- PR:L & UI:R highly negatively correlated\n",
    "- PR:N & UI:R highly correlated\n",
    "- PR:H & S:C highly correlated\n",
    "- I:L & S:C very highly correlated\n",
    "- A:N & S:C very highly correlated\n",
    "- C:* & I* extemely correlated both positively and negatively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import itertools\n",
    "\n",
    "crosstabs = {}\n",
    "\n",
    "def perform_independence_test(df: pd.DataFrame):\n",
    "    for c0, c1 in itertools.combinations(metrics.keys(), 2):\n",
    "        xtab = pd.crosstab(df[c0], df[c1])\n",
    "        crosstabs[\":\".join((c0,c1))] = xtab\n",
    "        print(f\"\\n=== {c0} & {c1}\")\n",
    "        print(sm.stats.Table(xtab).resid_pearson)\n",
    "\n",
    "perform_independence_test(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for statistical significance of cross-category dependence\n",
    "\n",
    "$H_{0_{\\alpha, \\beta}}$: metric $\\alpha$ and metric $\\beta$ are independent\n",
    "\n",
    "Use standard significance level $\\alpha = 0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "xtab = crosstabs[\"C:I\"]\n",
    "alpha = 0.5\n",
    "chi2stat, pvalue, dof, expected_frequency = scipy.stats.chi2_contingency(xtab)\n",
    "chi2stat, pvalue, dof, expected_frequency, pvalue <= alpha"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding CVSS Vectors\n",
    "\n",
    "The following table lists out the metrics comprised by the CVSS vector.\n",
    "More info can be found in the [CVSS v3.1 Specification Document](https://www.first.org/cvss/specification-document).\n",
    "Each metric contributes a predefined amount to the overall CVE CVSS score,\n",
    "and the score is completely determined by the values of these metrics.\n",
    "Values in the table below are ordered from most to least severe.\n",
    "\n",
    "| **Base metric** | **Base metric type** | **Description** | **Possible values** |\n",
    "|---|---|---|---|\n",
    "| Attack Vector (AV) | Exploitability | This metric reflects the context by which vulnerability exploitation is possible. | Network, Adjacent, Local, Physical |\n",
    "| Attack Complexity (AC) | Exploitability | This metric describes the conditions beyond the attacker’s control that must exist in order to exploit the vulnerability. | Low, High |\n",
    "| Privileges Required (PR) | Exploitability | This metric describes the level of privileges an attacker must possess before successfully exploiting the vulnerability. | None, Low, High |\n",
    "| User Interaction (UI) | Exploitability | This metric captures the requirement for a human user, other than the attacker, to participate in the successful compromise of the vulnerable component. | None, Required |\n",
    "| Scope (S) | Scope¹ | The Scope metric captures whether a vulnerability in one vulnerable component impacts resources in components beyond its security scope. | Changed, Unchanged |\n",
    "| Confidentiality (C) | Impact | This metric measures the impact to the confidentiality of the information resources managed by a software component due to a successfully exploited vulnerability. | High, Low, None |\n",
    "| Integrity (I) | Impact | This metric measures the impact to integrity of a successfully exploited vulnerability. | High, Low, None |\n",
    "| Availability (A) | Impact | This metric measures the impact to the availability of the impacted component resulting from a successfully exploited vulnerability. | High, Low, None |\n",
    "\n",
    "<br>\n",
    "\n",
    "> ¹Scope was introduced in CVSS3.1.\n",
    "\n",
    "For our model, the precise meaning of these metrics and their subcategories is unimportant.\n",
    "The relevant question is: _how independent are these metrics and subcategories?_\n",
    "\n",
    "Assumption of label independence certainly makes our job easier,\n",
    "as it leaves the door open for more basic machine learning algorithms\n",
    "like **multi-category logistic regression**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "# Generate two sets of data\n",
    "data1 = np.random.normal(0, 1, 1000)\n",
    "data2 = np.random.normal(0.1, 1, 1000)\n",
    "\n",
    "# Perform a t-test\n",
    "t_stat, p_value = scipy.stats.ttest_ind(data1, data2)\n",
    "\n",
    "print(f\"t-statistic: {t_stat}\")\n",
    "print(f\"p-value: {p_value}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to understand if the results in the crosstable are statistically significant\n",
    "Then, select an approach for training (multi-class log reg?)\n",
    "\n",
    "Then, try a basic run\n",
    "\n",
    "Ok, I understand `pvalue` enough to proceed.\n",
    "Let's find the p-values of all combinations\n",
    "and put it in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalues = {}\n",
    "def calculate_p_values(df: pd.DataFrame):\n",
    "    for c0, c1 in itertools.combinations(metrics.keys(), 2):\n",
    "        xtab = pd.crosstab(df[c0], df[c1])\n",
    "        chi2stat, pvalue, _, _ = scipy.stats.chi2_contingency(xtab)\n",
    "        pvalues[\".\".join((c0,c1))] = pvalue\n",
    "\n",
    "calculate_p_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "pt = PrettyTable()\n",
    "pt.field_names = [\"Metric Combination\", \"Independence p-value\"]\n",
    "pt.align = \"l\"\n",
    "\n",
    "for mc, pval in pvalues.items():\n",
    "    if pval < 0.05:\n",
    "        pval = \"<0.05 (!)\"\n",
    "    pt.add_row((mc, pval))\n",
    "\n",
    "pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistically_independent = len(list(filter(lambda p: p > 0.05, pvalues.values())))\n",
    "print(\n",
    "    f\"Only {statistically_independent} combinations are statistically independent out\"\n",
    "    f\" of {len(pvalues)} combinations of cvss metrics\"\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the data and algorithm\n",
    "\n",
    "To start, I will use the UI metric because\n",
    "1. it is binomial\n",
    "1. it has a good split between the two values\n",
    "\n",
    "I will use a simple MLE logistic regression with gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"UI\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.dropna(subset=[\"UI\"])\n",
    "X = create_bow(df_clean[\"processed_descs\"].dropna())\n",
    "# Absorb bias into X\n",
    "X = np.insert(X, 0, 1, axis=1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.where(df[\"UI\"].dropna() == \"Required\", 1, 0)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_split = 1000\n",
    "# Transpose X such that examples are column vectors\n",
    "X_train, X_test = X[tt_split:].T, X[:tt_split].T\n",
    "Y_train, Y_test = Y[tt_split:], Y[:tt_split]\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def init_params(n_features):\n",
    "    return np.zeros(n_features)\n",
    "\n",
    "\n",
    "def num_incorrect(labels: np.ndarray, predictions: np.ndarray) -> np.ndarray:\n",
    "    c1 = (predictions > 0.5) & (labels == 1)\n",
    "    c2 = (predictions < 0.5) & (labels == 0)\n",
    "\n",
    "    return np.logical_or(c1, c2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "grad_desc_cycles = 300\n",
    "w = init_params(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(grad_desc_cycles):\n",
    "    z = w.dot(X_train)\n",
    "    predictions = sigmoid(z)\n",
    "    errors = Y_train - predictions\n",
    "    grad = errors.dot(X_train.T) / len(Y_train)\n",
    "    w = w + alpha * grad\n",
    "\n",
    "    if i % 10: continue\n",
    "    predictions_test = sigmoid(w.dot(X_test))\n",
    "    n_correct = np.count_nonzero(num_incorrect(Y_test, predictions_test))\n",
    "    print(np.around(predictions, 3), np.sum(errors), f\"({n_correct} / {len(Y_test)})\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Logistic Regression\n",
    "Now let's try a different metric, `C` (Confidentiality).\n",
    "This metric has 3 possible values, so I'll need to use\n",
    "Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "metric = \"AV\"\n",
    "\n",
    "df_clean = df.dropna(subset=[metric])\n",
    "X = create_bow(df_clean[\"processed_descs\"].dropna())\n",
    "# Absorb bias into X\n",
    "X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "Y = OneHotEncoder(sparse_output=False).fit_transform(\n",
    "    df[metric].dropna().to_numpy().reshape(-1, 1)\n",
    ")\n",
    "\n",
    "X.shape, Y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[metric].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_split = 1000\n",
    "# Transpose X such that examples are column vectors\n",
    "X_train, X_test = X[tt_split:].T, X[:tt_split].T\n",
    "Y_train, Y_test = Y[tt_split:], Y[:tt_split]\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z: np.ndarray):\n",
    "    return np.exp(Z) / np.sum(np.exp(Z), axis=0)\n",
    "\n",
    "\n",
    "def sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def init_params(n_features, n_categories):\n",
    "    return np.zeros((n_features, n_categories))\n",
    "\n",
    "def categorical_cross_entropy_loss(ohY: np.ndarray, A: np.ndarray) -> float:\n",
    "    assert np.all(\n",
    "        (ohY.sum(axis=1) == 1) & np.all((ohY == 0) | (ohY == 1), axis=1)\n",
    "    )  # one-hot encoding\n",
    "    EPSILON = 1e-8 # avoid math exceptions if A happens to contain 0\n",
    "    return -np.mean(np.sum(ohY * np.log(A + EPSILON), axis=1), dtype=float)\n",
    "\n",
    "def num_correct(Y: np.ndarray, A: np.ndarray) -> int:\n",
    "    return np.sum(np.argmax(Y, axis=1) == np.argmax(A, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = init_params(len(Y_train.T), len(X_train))\n",
    "alpha = 0.11\n",
    "grad_desc_cycles = 200\n",
    "\n",
    "for i in range(grad_desc_cycles):\n",
    "    Z = W.dot(X_train)\n",
    "    predictions = softmax(sigmoid(Z))\n",
    "    errors = Y_train.T - predictions\n",
    "    grad = errors.dot(X_train.T) / len(Y_train)\n",
    "    W = W + alpha * grad\n",
    "\n",
    "    if i % 10: continue\n",
    "    print(num_correct(Y_train, predictions.T), len(Y_train))\n",
    "    print(categorical_cross_entropy_loss(Y_train, predictions.T))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|metric|best result|\n",
    "|---|---|\n",
    "|C\t| 1608 |\n",
    "|I\t| 1581 |\n",
    "|AV\t| 1897 |\n",
    "|UI\t| 1895 |\n",
    "|S\t| 2211 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch\n",
    "\n",
    "I now want to explore using the same techniques,\n",
    "but with a purpose-built library like `pytorch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "metric = \"AV\"\n",
    "\n",
    "df_clean = df.dropna(subset=[metric])\n",
    "X = create_bow(df_clean[\"processed_descs\"].dropna())\n",
    "# Absorb bias into X\n",
    "# X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "Y = OneHotEncoder(sparse_output=False).fit_transform(\n",
    "    df[metric].dropna().to_numpy().reshape(-1, 1)\n",
    ").argmax(axis=1)\n",
    "\n",
    "X.shape, Y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# float32 and long are the usual data types\n",
    "# for features and labels in PyTorch\n",
    "X_train_torch = torch.from_numpy(X).float()\n",
    "Y_train_torch = torch.from_numpy(Y).long()\n",
    "X_train_torch.shape, Y_train_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_torch.dtype, Y_train_torch.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(X_train_torch.size(1), len(Y_train_torch.unique()))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.11)\n",
    "# Combines LogSoftMax and NLLLoss\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 228\n",
    "\n",
    "for i in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_torch)\n",
    "\n",
    "    loss = loss_fn(outputs, Y_train_torch)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 10: continue\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    correct = (predicted == Y_train_torch).sum().item()\n",
    "    print(f\"Epoch {i} Accuracy: {correct/len(Y_train_torch)} Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"[%(levelname)-8s] (%(name)s) %(message)s\",\n",
    "    level=logging.DEBUG,\n",
    ")\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class CVSSMetricType(Enum):\n",
    "    Exploit = \"Exploit\"\n",
    "    Impact = \"Impact\"\n",
    "    Scope = \"Scope\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CVSSMetricMeta:\n",
    "    type: CVSSMetricType\n",
    "    abbrev: str\n",
    "    name: str\n",
    "    # ordered from higher to lower CVSS Score weighting\n",
    "    categories: list[str]\n",
    "\n",
    "\n",
    "CVSS_BASE_METRICS = {\n",
    "    \"AV\": CVSSMetricMeta(\n",
    "        CVSSMetricType.Exploit,\n",
    "        \"AV\",\n",
    "        \"Attack Vector\",\n",
    "        [\"Network\", \"Adjacent\", \"Local\", \"Physical\"],\n",
    "    ),\n",
    "    \"AC\": CVSSMetricMeta(\n",
    "        CVSSMetricType.Exploit, \"AC\", \"Attack Complexity\", [\"Low\", \"High\"]\n",
    "    ),\n",
    "    \"PR\": CVSSMetricMeta(\n",
    "        CVSSMetricType.Exploit, \"PR\", \"Privileges Required\", [\"None\", \"Low\", \"High\"]\n",
    "    ),\n",
    "    \"UI\": CVSSMetricMeta(\n",
    "        CVSSMetricType.Exploit, \"UI\", \"User Interaction\", [\"None\", \"Required\"]\n",
    "    ),\n",
    "    \"S\": CVSSMetricMeta(CVSSMetricType.Scope, \"S\", \"Scope\", [\"Changed\", \"Unchanged\"]),\n",
    "    \"C\": CVSSMetricMeta(\n",
    "        CVSSMetricType.Impact, \"C\", \"Confidentiality\", [\"High\", \"Low\", \"None\"]\n",
    "    ),\n",
    "    \"I\": CVSSMetricMeta(\n",
    "        CVSSMetricType.Impact, \"I\", \"Integrity\", [\"High\", \"Low\", \"None\"]\n",
    "    ),\n",
    "    \"A\": CVSSMetricMeta(\n",
    "        CVSSMetricType.Impact, \"A\", \"Availability\", [\"High\", \"Low\", \"None\"]\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "class CVEEngineModel:\n",
    "    def __init__(self, n_features: int):\n",
    "        # assume same learn rate for each metric\n",
    "        self.learn_rate = 0.10\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.training_epochs = 1000\n",
    "\n",
    "        self.models = {}\n",
    "        self.optimizers = {}\n",
    "\n",
    "        for metric_meta in CVSS_BASE_METRICS.values():\n",
    "            model = nn.Linear(n_features, len(metric_meta.categories))\n",
    "            self.models[metric_meta.abbrev] = model\n",
    "            self.optimizers[metric_meta.abbrev] = optim.SGD(\n",
    "                model.parameters(), lr=self.learn_rate\n",
    "            )\n",
    "        assert self.models.keys() == self.optimizers.keys()\n",
    "\n",
    "    def display_parameters(self):\n",
    "        print(\"== models ==\")\n",
    "        for metric, model in self.models.items():\n",
    "            print(f\"metric: {metric}\\tnumber of categories: {model.out_features}\")\n",
    "\n",
    "        print(\"\\n== parameters ==\")\n",
    "        print(f\"learn rate: {self.learn_rate}\")\n",
    "        print(f\"num features: {list(self.models.values())[0].in_features}\")\n",
    "\n",
    "    def _train_metric(self, X_train: torch.Tensor, Y_train: torch.Tensor, metric: str):\n",
    "        for i in range(self.training_epochs):\n",
    "            self.optimizers[metric].zero_grad()\n",
    "            outputs = self.models[metric](X_train)\n",
    "\n",
    "            loss = self.loss_fn(outputs, Y_train)\n",
    "            loss.backward()\n",
    "\n",
    "            self.optimizers[metric].step()\n",
    "\n",
    "            if i % 100:\n",
    "                continue\n",
    "            log.debug(f\"metric: {metric}\\tepoch: {i:2}\\tloss: {loss}\")\n",
    "\n",
    "    def _validate_Y_properties(self, Y_train: torch.Tensor):\n",
    "        assert Y_train.shape[1] == len(self.models)\n",
    "        values, _ = torch.max(Y_train, dim=0)\n",
    "        assert list(values) == [len(metric_meta.categories) - 1 for metric_meta in CVSS_BASE_METRICS.values()]\n",
    "            \n",
    "\n",
    "    def train_all(self, X_train: torch.Tensor, Y_train: torch.Tensor):\n",
    "        \"\"\"Trains all models on the provided training data.\n",
    "        :param metric_labels: a map from CVSS metric metriciations\n",
    "                              to the Y_train torch.Tensor that is\n",
    "                              associated with X_train.\n",
    "        \"\"\"\n",
    "        self._validate_Y_properties(Y_train)\n",
    "\n",
    "        for i, metric in enumerate(self.models.keys()):\n",
    "            log.debug(f\"++ training metric {i}: {metric}\")\n",
    "            self._train_metric(X_train, Y_train[:,i], metric)\n",
    "\n",
    "    def predict(self, X: torch.Tensor) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Returns predictions and confidence scores\n",
    "        indexed by cvss metric\"\"\"\n",
    "\n",
    "        predictions = np.zeros((X.shape[0], len(self.models)))\n",
    "        confidence_scores = np.zeros((X.shape[0], len(self.models)))\n",
    "\n",
    "        for i, (metric, model) in enumerate(self.models.items()):\n",
    "            prob = nn.functional.softmax(model(X), dim=1)\n",
    "\n",
    "            pred = torch.argmax(prob, dim=1)\n",
    "\n",
    "            confidence = prob[range(prob.shape[0]), pred]\n",
    "\n",
    "            predictions[:, i] = pred.numpy()\n",
    "            confidence_scores[:, i] = confidence.detach().numpy()\n",
    "\n",
    "        assert predictions.shape == confidence_scores.shape\n",
    "        return predictions, confidence_scores\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_accuracy(Y_true: np.ndarray, Y_pred: np.ndarray):\n",
    "        \"\"\"Computes the accuracy for each metric\n",
    "        by measuring the proportion of correct predictions\"\"\"\n",
    "        assert Y_true.shape == Y_pred.shape\n",
    "        return np.mean(Y_true == Y_pred, axis=0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "My next task is to do a full run of training\n",
    "and then print out the results on the test data.\n",
    "\n",
    "I want to see a table like this:\n",
    "\n",
    "- av: (45 / 100)\n",
    "- i: (77 / 100)\n",
    "- ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: prepare labels and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# pick any metric to remove NaNs\n",
    "df_clean = df.dropna(subset=[\"AV\"]).copy()\n",
    "\n",
    "for metric in CVSS_BASE_METRICS.keys():\n",
    "    encoder = LabelEncoder()\n",
    "    df_clean[metric] = encoder.fit_transform(df[metric].dropna())\n",
    "\n",
    "Y_np = df_clean[list(CVSS_BASE_METRICS.keys())].values\n",
    "Y = torch.from_numpy(Y_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np = create_bow(df_clean[\"processed_descs\"])\n",
    "X = torch.from_numpy(X_np).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 0.8\n",
    "i = int(0.8 * len(X))\n",
    "X_train, X_test = X[:i], X[i:]\n",
    "Y_train, Y_test = Y[:i], Y[i:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvem = CVEEngineModel(X_train.shape[1])\n",
    "cvem.display_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvem.train_all(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, cs = cvem.predict(X_test)\n",
    "pred, cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pct correct\n",
    "np.mean(Y_test.numpy() == pred, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average confidence scores\n",
    "np.mean(cs, axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some thoughts\n",
    "\n",
    "- Want to allow for a \"human-in-the-loop\" feedback mechanism\n",
    "- Using \"active learning\", the model asks for feedback when it is less confident\n",
    "- Probably what I would use is closer to \"online learning\", where I can simply\n",
    "  scrape the data after the fact and retrain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
