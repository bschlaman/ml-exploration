{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of Gradient Descent and Neural Networks\n",
    "\n",
    "#### Brendan Schlaman (July 2023)\n",
    "\n",
    "**Technical Requirements:**\n",
    "- Jupyter (with MathJax support for $\\LaTeX$)\n",
    "- `python>=3.9`\n",
    "- `numpy`\n",
    "- `prettytable`\n",
    "\n",
    "**Human Requirements:**\n",
    "- Reasonable understanding of linear algebra and vector calculus\n",
    "\n",
    "---\n",
    "\n",
    "### Introduction\n",
    "\n",
    "The purpose of this notebook is to incrementally translate the mathematics of\n",
    "gradient descent into code in the context of a simple neural network.\n",
    "\n",
    "We will use only simple math libraries like `numpy` to gain a more fundamental intuition of the concepts; these concepts are\n",
    "abstracted away in more purpose-built libraries like `pytorch` or TensorFlow.\n",
    "\n",
    "We will start with the simplest possible neural network: a 1-wide ($m=1$), single layer ($L = 1$) network, and build up from there."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation\n",
    "\n",
    "The following conventions will be used:\n",
    "\n",
    "| Variable or symbol | Definition |\n",
    "|---|---|\n",
    "| $\\mathbf{W}^{(l)}$ | The weights matrix that describes inputs to layer $l$.  $\\mathbf{W}^{(l)}$ is an $(m \\times n)$ matrix, where $m$ is the size of layer $l$, and $n$ is the size of layer $l-1$. |\n",
    "| $\\mathbf{w}$ | Same as above, but when $\\mathbf{w}$ is a vector (single column matrix).  This is the case where there is a single neuron for a particular layer. |\n",
    "| `W1` | When using a fixed number of layers *L*, `W1` will be the weights feeding into the first hidden layer (or the output layer).  `Wl` in the code corresponds to $w^{(l)}$ in the math notation. |\n",
    "| $\\mathbf{b}$ | A single bias matrix that contributes to a hidden layer.  $\\mathbf{b}$ is a $(m \\times 1)$ matrix, where $m$ is the size (width) of the associated hidden layer.  |\n",
    "| `b1` | When using a fixed number of layers *L*, `b1` will be the weights feeding into the first hidden layer (or the output layer).  `bn` in the code corresponds to $b^{(n)}$ in the math notation. |\n",
    "| `z` | A preactivated layer.  It is the result of the linear combination (weights times inputs plus bias) before an activation function is applied. |\n",
    "| `a` | An activated layer, i.e. $\\sigma (z)$.  The activated layer becomes the input to the next layer. |\n",
    "| $\\mathbf{X}$ | The training examples matrix.  Each row represents an example, and each column represents a feature. |\n",
    "| $\\mathbf{x}$ | A single input vector to a layer $l$; $z^{(l)} = f^{(l)}(\\mathbf{x})$.  |\n",
    "| $\\mathbf{Y}$ | The training targets.  $\\mathbf{Y_i}$ is the target of example $\\mathbf{X_i}$.  The columns represent the target probability distribution over the categories of the data.  In our case, this will be a *one-hot* encoding of the expected label. |\n",
    "| $L$ | The depth of the neural network; the number of hidden layers plus the output layer. |\n",
    "| $C_i$ | The cost of the neural network for training example $i$. |\n",
    "| $j$ | Index of the current layer, the layer denoted in the superscript. |\n",
    "| $k$ | Index of the previous layer, the layer before the layer denoted in the superscript. |\n",
    "\n",
    "<br>\n",
    "\n",
    "> Note that the *input layer* is treated as the “0th” layer and does not have wieghts or biases.\n",
    "> Imagine each layer $l$ \"owning\" the weights $\\mathbf{W}^{(l)}$ and biases $\\mathbf{b}^{(l)}$\n",
    "> that contribute to its activation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on NN architecture and dimensionality\n",
    "\n",
    "One of the things that confused me the most as I first got into neural networks is\n",
    "how the 4 \"dimensionalities\" of neural network training (3 *architecture dimensions* x num examples)\n",
    "should be represented in your datastructures.  Luckily, math should work out the same for whatever\n",
    "configuration you choose, as long as that configuration is consistent, but I believe some representations\n",
    "are better (and indeed perhaps more performant) than others.  I landed on the following\n",
    "representation after some deliberation and consultation with online sources, and\n",
    "I think it does the best job of preserving intuitions while following convention.\n",
    "Just be aware that other materials, tutorials, or libraries may structure their data differently.\n",
    "\n",
    "> Note that these dimensionalities are separate from the **hyperparameters** of the system,\n",
    "> like the learn rate and choice of activation function(s).  For our exmples, the\n",
    "> representations for these parameters will be clear in the code.\n",
    "\n",
    "| Parameter Type | Dimension | Our notation | Datastructure representation |\n",
    "|---|---|---|---|\n",
    "| Input | Number of training examples | $N$ | Num rows in $\\mathbf{X}$ and $\\mathbf{Y}$ |\n",
    "| Architecture | Depth of the NN | $L$ | Object of iteration.  We will iterate over each layer during a single forward or back propagagation cycle (or unroll and compute sequentially if $L$ is sufficiently small, using unique variables `Wl`, `bl`, `zl`, `al`, etc, where `l` $\\in \\{1,\\dots,L\\}$). |\n",
    "| Architecture | Size of layer $l$ | $m^{(l)}$ | Num rows of $\\mathbf{W}^{(l)}$ |\n",
    "| Architecture (multiple values) | Size of layer $l-1$ | $m^{(l-1)}$ | Num columns of $\\mathbf{W}^{(l)}$ |\n",
    "| Input + Architecture¹ | Number of features; size of input layer | $m^{(0)}$ | Num columns of $\\mathbf{X}$ |\n",
    "| Input + Architecture¹ | Number of categories; size of output layer | $m^{(L)}$ | Num columns of $\\mathbf{Y}$; size of $\\mathbf{b}^{(L)}$; num rows of $\\mathbf{W}^{(L)}$ |\n",
    "\n",
    "<br>\n",
    "\n",
    "> ¹These parameters can be inferred from the 3 *architecture* parameters above them,\n",
    "> i.e. they are a restating of the previous parameters.\n",
    "> I've included them here for illustration purposes.\n",
    "\n",
    "For example, one could easily imagine iterating over examples $\\mathbf{x_i}$\n",
    "and keeping track of a map from layer $l$ to parameter\n",
    "$\\mathbf{W}^{(l)}, \\mathbf{b}^{(l)}, \\mathbf{z}^{(l)}, \\mathbf{a}^{(l)}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key equations\n",
    "\n",
    "#### Neural network definitions\n",
    "\n",
    "$$\n",
    "    \\mathbf{z}^{(L)} = \\mathbf{W}^{(L)}\\mathbf{a}^{(L-1)} + \\mathbf{b}^{(L)}\n",
    "$$\n",
    "$$\n",
    "    \\mathbf{a}^{(L)} = \\sigma (\\mathbf{z}^{(L)})\n",
    "$$\n",
    "\n",
    "> Going forward, I'll drop the boldface notation for vectors and matrices.  $W$ is always a matrix, and $b$, $z$, $w$ are always vectors.\n",
    "\n",
    "\n",
    "#### Computing loss\n",
    "\n",
    "We'll use the common **Mean Squared Error (MSE)** loss function:\n",
    "\n",
    "$$\n",
    "    C_i = \\frac{1}{N}\\sum_{j=0}^{N-1}(y_j - a_j^{(L)})^2\n",
    "$$\n",
    "\n",
    "where $C_i$ is the loss from a single example, $x_i$.\n",
    "This is precicely the function we will try to minimize with gradient descent.\n",
    "Note that $\\frac{1}{N}$ simply scales the loss and does not change which inputs minimize the function.\n",
    "\n",
    "We are interested in how the cost of the network changes with respect to it's inputs, $W$ and $b$.\n",
    "The chain rule gives us our answer:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial C_i}{\\partial W_{jk}^{(l)}} =\n",
    "        \\frac{\\partial C_i}{\\partial a_j^{(l)}}\n",
    "        \\frac{\\partial a_j^{(l)}}{\\partial z_j^{(l)}}\n",
    "        \\frac{\\partial z_j^{(l)}}{\\partial W_{jk}^{(l)}}\n",
    "$$\n",
    "$$\n",
    "    \\frac{\\partial C_i}{\\partial b_j^{(l)}} =\n",
    "        \\frac{\\partial C_i}{\\partial a_j^{(l)}}\n",
    "        \\frac{\\partial a_j^{(l)}}{\\partial z_j^{(l)}}\n",
    "        \\frac{\\partial z_j^{(l)}}{\\partial b_j^{(l)}}\n",
    "$$\n",
    "\n",
    "It's also helpful to consider the derivative of $C_i$ with respect to $a_j^{(l)}$,\n",
    "since through that derivative, we get access future layers' ($l + 1, 2, \\dots$) weights and biases.\n",
    "Notice that the cost function is influenced in $n_{l+1}$ different ways for each $a_j^{(l)}$,\n",
    "so we use the *multivariable chain rule* to account for all the paths between $a_j^{(l)}$ and the next layer.\n",
    "Remember that when context switches from $l$ to $l + 1$, the indices $j, k$ are redefined in relation to that new context.\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial C_i}{\\partial a_j^{(l)}} =\n",
    "        \\sum_{j=0}^{n_{l+1}-1}\n",
    "        \\frac{\\partial C_i}{\\partial a_j^{(l+1)}}\n",
    "        \\frac{\\partial a_j^{(l+1)}}{\\partial z_j^{(l+1)}}\n",
    "        \\frac{\\partial z_j^{(l+1)}}{\\partial a_j^{(l)}}\n",
    "$$\n",
    "\n",
    "The total loss derivative with respect to $W^{(L)}$ is the average of the losses $C_i$ from all examples $x_i$.\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial C}{\\partial W^{(L)}} = \\frac{1}{n} \\sum_{i=0}^{n-1} \\frac{\\partial C_i}{\\partial W^{(L)}}\n",
    "$$\n",
    "\n",
    "The full gradient of the loss function over the entire network $\\nabla C$ comprises the above derivatives for all layers $l$.\n",
    "\n",
    "$$\n",
    "\\nabla C =\n",
    "    \\begin{bmatrix}\n",
    "    \\frac{\\partial C}{\\partial W^{(1)}} \\\\[1.5ex]\n",
    "    \\frac{\\partial C}{\\partial b^{(1)}} \\\\[1.5ex]\n",
    "    \\vdots \\\\[1.5ex]\n",
    "    \\frac{\\partial C}{\\partial W^{(L)}} \\\\[1.5ex]\n",
    "    \\frac{\\partial C}{\\partial b^{(L)}}\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Cross entropy loss:\n",
    "\n",
    "$C = -\\sum_{j=1}^{K} y_j \\log(\\hat{y}_j)$\n",
    "\n",
    "$C = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{K} y_{ij} \\log(\\hat{y}_{ij})$\n",
    "\n",
    "#### Nonlinear activation functions\n",
    "\n",
    "The standard softmax function $\\sigma : \\mathbb{R}^K \\mapsto (0, 1)^K; \\; K \\geq 1$:\n",
    "\n",
    "$$\n",
    "    \\sigma(\\mathbf{z})_i =\n",
    "        \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n",
    "        \\text{ for }\n",
    "        1, \\dots, K\n",
    "        \\text{ and }\n",
    "        \\mathbf{z} = (z_1, \\dots, z_K) \\in \\mathbb{R}^K\n",
    "$$\n",
    "\n",
    "> The symbols used above are consistent with most online materials,\n",
    "> where $i$ represents an index of the elements of $\\mathbf{z}$,\n",
    "> not an example index.\n",
    "\n",
    "For this neural network, we'll be using **ReLU** as our activation function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simpler cases\n",
    "\n",
    "It's helpful to look at some of the math in edge-case neural networks.\n",
    "\n",
    "In the case of a single output, our preactivation function can be written as\n",
    "\n",
    "$$\n",
    "    z = f(\\mathbf{a}) = \\mathbf{w} \\cdot \\mathbf{a} + b\n",
    "$$\n",
    "\n",
    "For a single layer ($L = 1$), 1-wide ($m = 1$) network, the partial derivatives of loss are:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial C_i}{\\partial w^{(L)}} =&&\n",
    "        \\frac{\\partial C_i}{\\partial a^{(L)}}\n",
    "        \\frac{\\partial a^{(L)}}{\\partial z^{(L)}}\n",
    "        \\frac{\\partial z^{(L)}}{\\partial w^{(L)}}\n",
    "        &= 2(a^{(L)} - y) \\sigma'(z^{(L)}) a^{(L-1)} \\\\\n",
    "    \\frac{\\partial C_i}{\\partial b^{(L)}} =&&\n",
    "        \\frac{\\partial C_i}{\\partial a^{(L)}}\n",
    "        \\frac{\\partial a^{(L)}}{\\partial z^{(L)}}\n",
    "        \\frac{\\partial z^{(L)}}{\\partial b^{(L)}}\n",
    "        &= 2(a^{(L)} - y) \\sigma'(z^{(L)}) \\\\\n",
    "    \\frac{\\partial C_i}{\\partial a^{(L-1)}} =&&\n",
    "        \\frac{\\partial C_i}{\\partial a^{(L)}}\n",
    "        \\frac{\\partial a^{(L)}}{\\partial z^{(L)}}\n",
    "        \\frac{\\partial z^{(L)}}{\\partial a^{(L-1)}}\n",
    "        &= 2(a^{(L)} - y) \\sigma'(z^{(L)}) w^{(L)}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-Wide, single layer Neural Network ($L = 1$; $m^{(L)} = 1$)\n",
    "\n",
    "Start with the simplest NN - an input layer connected to a single output node (one category).\n",
    "\n",
    "In this simple case, the output layer is simply\n",
    "\n",
    "$$\n",
    "    a(\\mathbf{x}) = σ(\\mathbf{w} \\cdot \\mathbf{x} + b)\n",
    "$$\n",
    "\n",
    "We can take advantage of vector calculus to find our partial derivatives, using the intermediate variable $z = \\mathbf{w} \\cdot \\mathbf{x} + b$.\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial a^{(l)}}{\\partial \\textbf{w}} =\n",
    "    \\frac{\\partial a^{(l)}}{\\partial z}\n",
    "    \\frac{\\partial z}{\\partial \\textbf{w}}\n",
    "$$\n",
    "\n",
    "Looking at $\\frac{\\partial z}{\\partial \\textbf{w}}$,\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial z}{\\partial \\textbf{w}}\n",
    "    &= \\frac{\\partial}{\\partial \\textbf{w}} \\left(\\text{sum} (\\mathbf{w} \\otimes \\mathbf{x}) + b\\right) \\\\\n",
    "    &= \\frac{\\partial \\ \\text{sum} (\\mathbf{v})}{\\partial \\textbf{v}} \\frac{\\partial \\mathbf{v}}{\\partial \\textbf{w}}\n",
    "    + \\frac{\\partial}{\\partial \\textbf{w}} b \\ ; \\ \\mathbf{v} = \\mathbf{w} \\otimes \\mathbf{x} \\\\\n",
    "    &= \\vec{1}^T \\text{diag}(\\mathbf{x}) + \\vec{0}^T \\\\\n",
    "    &= \\mathbf{x}^T\n",
    "\\end{align*}\n",
    "\n",
    "Finally, we have our answer:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial a^{(l)}}{\\partial \\textbf{w}} &= \\frac{\\partial a^{(l)}}{\\partial z} \\mathbf{x}^T \\\\\n",
    "    \\frac{\\partial a^{(l)}}{\\partial b} &= \\frac{\\partial a^{(l)}}{\\partial z}\n",
    "\\end{align*}\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial C}{\\partial b}\n",
    "    = \\frac{\\partial C}{\\partial a} \\frac{\\partial a}{\\partial z}\n",
    "    = - \\frac{\\partial a}{\\partial z} \\frac{2}{N}\\sum_i y_i - \\sigma(\\mathbf{w} \\cdot \\mathbf{x_i} + b)\n",
    "    = \\frac{\\partial a}{\\partial z} \\frac{2}{N}\\sum_i \\sigma(\\mathbf{w} \\cdot \\mathbf{x_i} + b) - y_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial C}{\\partial \\textbf{w}}\n",
    "    = \\frac{\\partial C}{\\partial a} \\frac{\\partial a}{\\partial z} \\mathbf{x}^T\n",
    "    = - \\frac{\\partial a}{\\partial z} \\frac{2}{N}\\sum_i (y_i - \\sigma(\\mathbf{w} \\cdot \\mathbf{x_i} + b)) \\mathbf{x_i}^T\n",
    "    = \\frac{\\partial a}{\\partial z} \\frac{2}{N}\\sum_i (\\sigma(\\mathbf{w} \\cdot \\mathbf{x_i} + b) - y_i) \\mathbf{x_i}^T \n",
    "$$\n",
    "\n",
    "\n",
    "With a learning rate of $\\alpha$, our gradient descent algorithm will look like:\n",
    "\n",
    "$$\n",
    "    \\mathbf{w}_{t+1} = \\mathbf{w}_t - \\alpha \\frac{\\partial C}{\\partial \\textbf{w}} \\\\[1.5ex]\n",
    "    b_{t+1} = b_t - \\alpha \\frac{\\partial C}{\\partial b}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "\n",
    "First we'll define some utility functions that help us build the data for our example.\n",
    "The nature of these functions is chosen arbitrarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def init_single_category_data(d: int):\n",
    "    \"\"\"Generates random training data with dimensionality `d`.\n",
    "    The label is also chosen arbitrarily.\n",
    "    \"\"\"\n",
    "    data_size = 10\n",
    "    labels = [9] * data_size  # randomly chose a desired label\n",
    "    examples = [[random.randrange(-9, 10) for _ in range(d)] for _ in range(data_size)]\n",
    "    return list(zip(examples, labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_dimensionality = 3\n",
    "\n",
    "examples, labels = zip(*init_single_category_data(input_dimensionality))\n",
    "X, Y = np.array(examples), np.array(labels)\n",
    "\n",
    "m1 = 1 # the width of our only layer! (remember, we don't count the input layer)\n",
    "\n",
    "def init_params():\n",
    "    w1 = np.random.randn(input_dimensionality) * 0.01\n",
    "    b1 = np.random.randn(m1) * 0.01\n",
    "    return w1, b1\n",
    "\n",
    "w1, b1 = init_params()\n",
    "\n",
    "X, Y, w1, b1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the same `ReLU` function throughout, regardless of layer width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(Z: np.ndarray) -> np.ndarray:\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def deriv_ReLU(Z: np.ndarray) -> np.ndarray:\n",
    "    return Z > 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward propagation is simple.\n",
    "Matrix multiply `w1` and `X.T` and add `b1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(\n",
    "    X: np.ndarray,\n",
    "    w1: np.ndarray,\n",
    "    b1: np.ndarray,\n",
    "):\n",
    "    Z1 = w1.dot(X.T) + b1\n",
    "    A1 = ReLU(Z1) # 1D ReLU\n",
    "    # print(w1, x, w1.dot(x), z1, a1)\n",
    "    return Z1, A1\n",
    "\n",
    "def mean_squared_error_loss(Y: np.ndarray, A: np.ndarray) -> float:\n",
    "    return np.sum(np.square(A-Y)) / len(A)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define a backpropagation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X: np.ndarray, Y: np.ndarray, Z: np.ndarray, A: np.ndarray):\n",
    "    n = len(Y)\n",
    "    dZ = 2*(A-Y) * deriv_ReLU(Z)\n",
    "    dW = 1/n * dZ.T.dot(X)\n",
    "    db = 1/n * np.sum(dZ)\n",
    "    return dW, db\n",
    "\n",
    "def descent(W1, b1, dW, db):\n",
    "    alpha = 0.01\n",
    "    W1 = W1 - alpha * dW\n",
    "    b1 = b1 - alpha * db\n",
    "    return W1, b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def pretty_print_parameters(**kwargs):\n",
    "    def yel(s: str) -> str:\n",
    "        return f\"\\033[93m{s}\\033[0m\"\n",
    "\n",
    "    pt = PrettyTable()\n",
    "    pt.field_names = [\"Parameter\", \"Value\"]\n",
    "    pt.max_width[\"Value\"] = 80\n",
    "    pt.align = \"l\"\n",
    "\n",
    "    for k, v in kwargs.items():\n",
    "        pt.add_row([yel(k), v])\n",
    "\n",
    "    print(pt)\n",
    "\n",
    "def run(X: np.ndarray, Y: np.ndarray):\n",
    "    W1, b1 = init_params()\n",
    "    for i in range(14000):\n",
    "        Z1, A1 = forward_prop(X, W1, b1)\n",
    "        loss = mean_squared_error_loss(Y, A1)\n",
    "        dW, db = gradient(X, Y, Z1, A1)\n",
    "        W1, b1 = descent(W1, b1, dW, db)\n",
    "        if i % 500 == 0:\n",
    "            print(f\"===== {i} ======\")\n",
    "            pretty_print_parameters(W1=W1, b1=b1, A1=A1, loss=loss, dW=dW, db=db)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(X, Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talk about overfitting!  The network has found a trivial solution to the problem: $\\mathbf{w} = \\vec{0} ; \\ b = 9$.\n",
    "\n",
    "Great to know it works!  Now let's try a slightly harder problem for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array([3, 4, 1, -8, -7, 4, 4, -8, 3, 1])\n",
    "run(X, Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it certainly converges; on what, I'm not sure.  Who knows if the minima is local or global; it's hard to tell when the training data $X$ was generated randomly.\n",
    "\n",
    "Time to step it up to the big leagues.  We will now increase two dimensions of our network: $L$ and $m^{(l > 0)}$.\n",
    "This means we can build a real, functional neural network!  We'll be using the classic **mnist dataset** for our final project.\n",
    "\n",
    "The network will be an $L = 3$ network (2 hidden layers, 1 output layer).  Layer sizes will be as follows:\n",
    "\n",
    "| Layer $l$ | Size |\n",
    "|---|---|\n",
    "| $m^{(1)}$ | 16 |\n",
    "| $m^{(2)}$ | 12 |\n",
    "| $m^{(L)}$ | 10 (defined by number of categories) |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, the code won't change too much.  The only differences are:\n",
    "\n",
    "1. Introduce parameter lists to hold weights, biases, preactivated neurons, and activated neurons for all layers.\n",
    "1. $Y$ becomes an $m \\times n$ matrix, where each column is a one-hot encoding of the value of the label for the $i$th example.\n",
    "1. $\\mathbf{b}^{(l)}$ becomes a vector instead of a number.\n",
    "1. For our final activation function, we'll use **softmax** instead of ReLU.\n",
    "   Softmax scales its input vector to a probability distribution while amplifying\n",
    "   differences in values.  Ideally, our model outputs a 1 for the correct label\n",
    "   and a 0 everywhere else.\n",
    "1. We'll switch to using **cross-entropy loss** for our loss function since it plays so nicely with **softmax**.\n",
    "\n",
    "\n",
    "Since we've introduced softmax, we'll need its derivative to perform gradient descent.\n",
    "\n",
    "Softmax is defined as follows:\n",
    "\n",
    "$$\n",
    "    \\text{softmax}(\\mathbf{z})_i = \\mathbf{\\sigma}_i(\\mathbf{z}) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n",
    "$$\n",
    "\n",
    "We'll find its Jacobian, leveraging the fact that outputs of softmax are strictly positive.\n",
    "(See Appendix for a wonderful intuition I discovered that helped me grasp the motivation behind Jacobian notation conventions.)\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial}{\\partial \\mathbf{z}} \\log (\\boldsymbol{\\sigma})\n",
    "    = \\frac{1}{\\mathbf{\\sigma}} \\frac{\\partial}{\\partial \\boldsymbol{\\sigma}} \\\\[1.5ex]\n",
    "    \\frac{\\partial}{\\partial \\boldsymbol{\\sigma}}\n",
    "    = \\boldsymbol{\\sigma} \\frac{\\partial}{\\partial \\mathbf{z}} \\log(\\boldsymbol{\\sigma})\n",
    "$$\n",
    "\n",
    "Focusing in on a single $\\sigma_i$ and $z_j$, we'll sum over $k$ to avoid polluting our namespace.\n",
    "\n",
    "$$\n",
    "    \\log(\\sigma_i) = z_j - \\log\\left(\\sum_k e^{z_k}\\right)\n",
    "$$\n",
    "\n",
    "We can write\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial}{\\partial z_j} \\log(\\sigma_i)\n",
    "    = 1\\{i = j\\}-\\frac{1}{\\sum_k e^{z_k}} \\frac{\\partial}{\\partial z_j} \\sum_k e^{z_k}\n",
    "    = 1\\{i=j\\}-\\sigma_j\n",
    "$$\n",
    "\n",
    "So finally,\n",
    "\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial \\sigma_i}{\\partial z_j}\n",
    "    = \\sigma_i \\frac{\\partial}{\\partial z_j} \\log(\\sigma_i)\n",
    "    = \\sigma_i (1\\{i=j\\}-\\sigma_j)\n",
    "$$\n",
    "\n",
    "$$\n",
    "    J_{\\text{softmax}} =\n",
    "\n",
    "    \\begin{bmatrix}\n",
    "    \\sigma_1(1-\\sigma_1) & -\\sigma_1\\sigma_2 & \\dots & -\\sigma_1\\sigma_n \\\\\n",
    "    -\\sigma_2\\sigma_1 & \\sigma_2(1-\\sigma_2) & \\dots & -\\sigma_2\\sigma_n \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    -\\sigma_m\\sigma_1 & -\\sigma_m\\sigma_2 & \\dots & \\sigma_m(1-\\sigma_m)\n",
    "    \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-entropy loss is defined as\n",
    "\n",
    "$$\n",
    "    L = -\\sum\\limits_{i}y_i \\log(\\sigma_i)\n",
    "$$\n",
    "\n",
    "And now, for the magic!\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial L}{\\partial z_j}\n",
    "    = -\\sum\\limits_{i}y_i \\frac{\\partial}{\\partial z_j} \\log(\\sigma_i)\n",
    "    = -\\sum\\limits_{i}y_i 1\\{i=j\\}-\\sigma_j \\\\\n",
    "    = \\sum\\limits_{i}y_i\\sigma_j - \\sum\\limits_{i}y_i 1\\{i=j\\}\n",
    "    \\frac{\\partial L}{\\partial z_j}\n",
    "    = \\left(\\sum\\limits_{i}y_i\\sigma_j\\right) - y_j\n",
    "    = \\sigma_j \\left(\\sum\\limits_{i}y_i\\right) - y_j\n",
    "    = \\sigma_j - y_j\n",
    "$$\n",
    "\n",
    "The last step is possible since the one-hot encoded $\\mathbf{Y}_i$ sums to 1.\n",
    "\n",
    "The entire expression simplifies beautifully:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial L}{\\partial \\mathbf{z}} = \\boldsymbol{\\sigma} - \\mathbf{Y}\n",
    "$$\n",
    "\n",
    "Now we're ready for the code!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the `softmax` and `one_hot` functions described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(A: np.ndarray):\n",
    "    return np.exp(A) / np.sum(np.exp(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(Y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Constructs a matrix whose rows represent\n",
    "    the target probability distribution of categories\n",
    "    for each row in Y, which in this case will be a\n",
    "    1 in the column indexed by the desired label,\n",
    "    and zeros everywhere else.\n",
    "    \"\"\"\n",
    "    one_hot_Y = np.zeros((Y.size, 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    return one_hot_Y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define our forward prop and loss functions.\n",
    "Note that the loss function is defined purely for debugging;\n",
    "we'll directly use its derivative during gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights: list[np.ndarray] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(\n",
    "    X: np.ndarray,\n",
    "    W1: np.ndarray,\n",
    "    b1: np.ndarray,\n",
    "):\n",
    "    Z1 = W1.dot(X.T) + b1\n",
    "    A1 = ReLU(Z1) # 1D ReLU\n",
    "    # print(w1, x, w1.dot(x), z1, a1)\n",
    "    return Z1, A1\n",
    "\n",
    "def mean_squared_error_loss(Y: np.ndarray, A: np.ndarray) -> float:\n",
    "    return np.sum(np.square(A-Y)) / len(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,5,2,2])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(softmax(a))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further reading and materials that helped me make this notebook\n",
    "\n",
    "- 3b1b series\n",
    "- tds - part 1 - 4\n",
    "- [Derivative of the Softmax Function and the Categorical Cross-Entropy Loss | by Thomas Kurbiel | Towards Data Science](https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A beautiful intuition behind the Jacobian\n",
    "\n",
    "I had a beautiful \"eureka\" moment today.  I understand the motivation behind the **numerator layout** of the Jacobian matrix.\n",
    "As a reminder, the Jacobian is defined as follows:\n",
    "\n",
    "$$\n",
    "    \\mathbf{J} \\mathbf{f}(\\mathbf{x}) = \\nabla \\mathbf{f}(\\mathbf{x}) =\n",
    "    \\begin{bmatrix}\n",
    "    \\nabla f_1(\\mathbf{x}) \\\\[1.5ex]\n",
    "    \\nabla f_2(\\mathbf{x}) \\\\[1.5ex]\n",
    "    \\nabla f_3(\\mathbf{x}) \\\\[1.5ex]\n",
    "    \\vdots \\\\[1.5ex]\n",
    "    \\nabla f_i(\\mathbf{x})\n",
    "    \\end{bmatrix}\n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "    \\frac{\\partial}{\\partial \\mathbf{x}} f_1(\\mathbf{x}) \\\\[1.5ex]\n",
    "    \\frac{\\partial}{\\partial \\mathbf{x}} f_2(\\mathbf{x}) \\\\[1.5ex]\n",
    "    \\frac{\\partial}{\\partial \\mathbf{x}} f_3(\\mathbf{x}) \\\\[1.5ex]\n",
    "    \\vdots \\\\[1.5ex]\n",
    "    \\frac{\\partial}{\\partial \\mathbf{x}} f_i(\\mathbf{x})\n",
    "    \\end{bmatrix}\n",
    "    = \n",
    "    \\begin{bmatrix}\n",
    "    \\frac{\\partial}{\\partial x_1} f_1(\\mathbf{x}) \\frac{\\partial}{\\partial x_2} f_1(\\mathbf{x}) \\dots \\frac{\\partial}{\\partial x_j} f_1(\\mathbf{x}) \\\\[1.5ex]\n",
    "    \\frac{\\partial}{\\partial x_1} f_2(\\mathbf{x}) \\frac{\\partial}{\\partial x_2} f_2(\\mathbf{x}) \\dots \\frac{\\partial}{\\partial x_j} f_2(\\mathbf{x}) \\\\[1.5ex]\n",
    "    \\frac{\\partial}{\\partial x_1} f_3(\\mathbf{x}) \\frac{\\partial}{\\partial x_2} f_3(\\mathbf{x}) \\dots \\frac{\\partial}{\\partial x_j} f_3(\\mathbf{x}) \\\\[1.5ex]\n",
    "    \\vdots \\\\[1.5ex]\n",
    "    \\frac{\\partial}{\\partial x_1} f_i(\\mathbf{x}) \\frac{\\partial}{\\partial x_2} f_i(\\mathbf{x}) \\dots \\frac{\\partial}{\\partial x_j} f_i(\\mathbf{x})\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The motivation for this configuration is clear when you consider the analog in linear transformations.\n",
    "Consider an $(m \\times n)$ matrix that defines a linear transformation from $\\mathbb{R}^n \\to \\mathbb{R}^m$.\n",
    "The columns are very interesting: the columns describe what happens to each unit vector in $\\mathbb{R}^n$ under the transformation.\n",
    "In other words, the element $x_{i,j}$ can be thought of as describing\n",
    "*what contribution to the $i$ component of output vectors does the $j$ component of input vectors make?*\n",
    "\n",
    "This aligns beautifully with the Jacobian!  $\\frac{\\partial}{\\partial x_j} f_i(\\mathbf{x})$ indeed describes\n",
    "*what contribution a small change in the $j$ component of an input vector makes to the $i$ component of the vector function $\\mathbf{f}$*.\n",
    "\n",
    "Isn't that satisfying!?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's my first pass at working out the Jacobian of softmax.\n",
    "It ends up being the same, but the logarithmic derivative is\n",
    "much more clever.\n",
    "\n",
    "$$\n",
    "    J \\boldsymbol{\\sigma} = \\frac{1}{\\left(\\sum_j e^{z_j}\\right)^2}\n",
    "\n",
    "    \\begin{bmatrix}\n",
    "    \\sum\\limits_{j \\neq 1} e^{z_j} & -e^{z_1 + z_2} & \\dots & -e^{z_1 + z_n} \\\\\n",
    "    -e^{z_2 + z_1} & \\sum\\limits_{j \\neq 2} e^{z_j} & \\dots & -e^{z_2 + z_n} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    -e^{z_m + z_1} & -e^{z_m + z_2} & \\dots & \\sum\\limits_{j \\neq m} e^{z_j}\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If we define $\\beta = \\sum_j e^{z_j}$, we have\n",
    "\n",
    "$$\n",
    "    J \\boldsymbol{\\sigma} = - \\frac{1}{\\beta^2}\n",
    "\n",
    "    \\begin{bmatrix}\n",
    "    e^{z_1} & e^{z_1 + z_2} & \\dots & e^{z_1 + z_n} \\\\\n",
    "    e^{z_2 + z_1} & e^{z_2} & \\dots & e^{z_2 + z_n} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    e^{z_m + z_1} & e^{z_m + z_2} & \\dots & e^{z_m}\n",
    "    \\end{bmatrix}\n",
    "    + \\beta \\mathbf{I}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
