{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of Gradient Descent and Neural Networks\n",
    "\n",
    "#### Brendan Schlaman (2023.07.01)\n",
    "\n",
    "**Technical Requirements:**\n",
    "- Jupyter (with MathJax support for $\\LaTeX$)\n",
    "- `numpy`\n",
    "\n",
    "**Human Requirements:**\n",
    "- Reasonable understanding of linear algebra and calculus\n",
    "\n",
    "---\n",
    "\n",
    "### Introduction\n",
    "\n",
    "The purpose of this notebook is to incrementally translate the mathematics of\n",
    "gradient descent into code in the context of a simple neural network.\n",
    "\n",
    "We will use only simple math libraries like `numpy` to gain a more fundamental intuition of the concepts; these concepts are\n",
    "abstracted away in more purpose-built libraries like `pytorch` or TensorFlow.\n",
    "\n",
    "We will start with the simplest possible neural network: a 1-wide ($m=1$), single layer ($L = 1$) network, and build up from there."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation\n",
    "\n",
    "The following conventions will be used:\n",
    "\n",
    "| Variable or symbol | Definition |\n",
    "|---|---|\n",
    "| $\\mathbf{W}$ | A single weights matrix that feeds into a hidden layer.  $W$ is an $(m \\times n)$ matrix, where $n$ is the width (size) of the inputs, and $m$ is the size of the outputs (the width of the hidden layer). |\n",
    "| $\\mathbf{w}$ | Same as above, but when $w$ is a vector (single column matrix).  This is the case where there is a single neuron for a particular layer. |\n",
    "| `W1` | When using a fixed number of layers *L*, `W1` will be the weights feeding into the first hidden layer (or the output layer).  `Wn` in the code corresponds to $w^{(n)}$ in the math notation. |\n",
    "| $\\mathbf{b}$ | A single bias matrix that contributes to a hidden layer.  $\\mathbf{b}$ is a $(m \\times 1)$ matrix, where $m$ is the size (width) of the associated hidden layer.  |\n",
    "| `b1` | When using a fixed number of layers *L1*, `b1` will be the weights feeding into the first hidden layer (or the output layer).  `bn` in the code corresponds to $b^{(n)}$ in the math notation. |\n",
    "| `z` | A preactivated layer.  It is the result of the linear combination (weights times inputs plus bias) before an activation function is applied. |\n",
    "| `a` | An activated layer, i.e. $\\sigma (z)$.  The activated layer becomes the input to the next layer. |\n",
    "| $L$ | The depth of the neural network; the number of hidden layers plus the output layer. |\n",
    "| $C_i$ | The cost of the neural network for training example $i$. |\n",
    "| $j$ | Index of the current layer, the layer denoted in the superscript. |\n",
    "| $k$ | Index of the previous layer, the layer before the layer denoted in the superscript. |\n",
    "\n",
    "<br>\n",
    "\n",
    "> Note that when using a fixed number of layers *L > 1*, the *input layer* is treated as the “0th” layer and does not have wieghts or biases.\n",
    "Think of this like each layer owning the weights and biases that feed into it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key equations\n",
    "\n",
    "#### Neural network definitions\n",
    "\n",
    "$$\n",
    "    \\mathbf{z}^{(L)} = \\mathbf{W}^{(L)}\\mathbf{a}^{(L-1)} + \\mathbf{b}^{(L)}\n",
    "$$\n",
    "$$\n",
    "    \\mathbf{a}^{(L)} = \\sigma (\\mathbf{z}^{(L)})\n",
    "$$\n",
    "\n",
    "> Going forward, I'll drop the boldface notation for vectors and matrices.  $W$ is always a matrix, and $b$, $z$, $w$ are always vectors.\n",
    "\n",
    "\n",
    "#### Computing loss\n",
    "\n",
    "We'll use the common **Mean Squared Error(MSE)** loss function:\n",
    "\n",
    "$$\n",
    "    C_i = \\frac{1}{n_L}\\sum_{j=0}^{n_L-1}(y_j - a_j^{(L)})^2\n",
    "$$\n",
    "\n",
    "where $C_i$ is the loss from a single example, $x_i$.  This is precicely the function we will try to minimize with gradient descent.  Note that $\\frac{1}{n_L}$ simply scales the loss and does not change which inputs minimize the function.\n",
    "\n",
    "We are interested in how the cost of the network changes with respect to it's inputs, $w$ and $b$.  The chain rule gives us our answer:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial C_i}{\\partial W_{jk}^{(l)}} =\n",
    "        \\frac{\\partial C_i}{\\partial a_j^{(l)}}\n",
    "        \\frac{\\partial a_j^{(l)}}{\\partial z_j^{(l)}}\n",
    "        \\frac{\\partial z_j^{(l)}}{\\partial W_{jk}^{(l)}}\n",
    "$$\n",
    "$$\n",
    "    \\frac{\\partial C_i}{\\partial b_j^{(l)}} =\n",
    "        \\frac{\\partial C_i}{\\partial a_j^{(l)}}\n",
    "        \\frac{\\partial a_j^{(l)}}{\\partial z_j^{(l)}}\n",
    "        \\frac{\\partial z_j^{(l)}}{\\partial b_j^{(l)}}\n",
    "$$\n",
    "\n",
    "It's also helpful to consider the derivative of $C_i$ with respect to $a_j^{(l)}$,\n",
    "since through that derivative, we get access future layers ($l + 1, 2, \\dots$) weights and biases.\n",
    "Notice that the cost function is influenced in $n_{l+1}$ different ways for each $a_j^{(l)}$,\n",
    "so we use the *multivariable chain rule* to account for all the paths between $a_j^{(l)}$ and the next layer.\n",
    "Remember that when context switches from $l$ to $l + 1$, the indices $j, k$ are redefined in relation to that new context.\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial C_i}{\\partial a_j^{(l)}} =\n",
    "        \\sum_{j=0}^{n_{l+1}-1}\n",
    "        \\frac{\\partial C_i}{\\partial a_j^{(l+1)}}\n",
    "        \\frac{\\partial a_j^{(l+1)}}{\\partial z_j^{(l+1)}}\n",
    "        \\frac{\\partial z_j^{(l+1)}}{\\partial a_j^{(l)}}\n",
    "$$\n",
    "\n",
    "The total loss derivative with respect to $W^{(L)}$ is the average of the losses $C_i$ from all examples $x_i$.\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial C}{\\partial W^{(L)}} = \\frac{1}{n} \\sum_{i=0}^{n-1} \\frac{\\partial C_i}{\\partial W^{(L)}}\n",
    "$$\n",
    "\n",
    "The full gradient of the loss function over the entire network $\\nabla C$ comprises the above derivatives across all layers.\n",
    "\n",
    "$$\n",
    "\\nabla C =\n",
    "    \\begin{bmatrix}\n",
    "    \\frac{\\partial C}{\\partial W^{(1)}} \\\\[1.5ex]\n",
    "    \\frac{\\partial C}{\\partial b^{(1)}} \\\\[1.5ex]\n",
    "    \\vdots \\\\[1.5ex]\n",
    "    \\frac{\\partial C}{\\partial W^{(L)}} \\\\[1.5ex]\n",
    "    \\frac{\\partial C}{\\partial b^{(L)}}\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Cross entropy loss:\n",
    "\n",
    "$C = -\\sum_{j=1}^{K} y_j \\log(\\hat{y}_j)$\n",
    "\n",
    "$C = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{K} y_{ij} \\log(\\hat{y}_{ij})$\n",
    "\n",
    "#### Nonlinear activation functions\n",
    "\n",
    "The standard softmax function $\\sigma : \\mathbb{R}^K \\mapsto (0, 1)^K; \\; K \\geq 1$:\n",
    "\n",
    "$$\n",
    "    \\sigma(\\mathbf{z})_i =\n",
    "        \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n",
    "        \\text{ for }\n",
    "        1, \\dots, K\n",
    "        \\text{ and }\n",
    "        \\mathbf{z} = (z_1, \\dots, z_K) \\in \\mathbb{R}^K\n",
    "$$\n",
    "\n",
    "> The symbols used above are consistent with most online materials, where $i$ represents an index of the elements of $\\mathbf{z}$, not an example index.\n",
    "\n",
    "For this neural network, we'll be using ReLU as our activation function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simpler cases\n",
    "\n",
    "It's helpful to look at some of the math in edge-case neural networks.\n",
    "\n",
    "In the case of a single output, our preactivation function can be written as\n",
    "\n",
    "$$\n",
    "    z = f(\\mathbf{a}) = \\mathbf{w} \\cdot \\mathbf{a} + b\n",
    "$$\n",
    "\n",
    "For a single layer ($L = 1$), 1-wide ($m = 1$) network, the partial derivatives of loss are:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial C_i}{\\partial w^{(L)}} =&&\n",
    "        \\frac{\\partial C_i}{\\partial a^{(L)}}\n",
    "        \\frac{\\partial a^{(L)}}{\\partial z^{(L)}}\n",
    "        \\frac{\\partial z^{(L)}}{\\partial w^{(L)}}\n",
    "        &= 2(a^{(L)} - y) \\sigma'(z^{(L)}) a^{(L-1)} \\\\\n",
    "    \\frac{\\partial C_i}{\\partial b^{(L)}} =&&\n",
    "        \\frac{\\partial C_i}{\\partial a^{(L)}}\n",
    "        \\frac{\\partial a^{(L)}}{\\partial z^{(L)}}\n",
    "        \\frac{\\partial z^{(L)}}{\\partial b^{(L)}}\n",
    "        &= 2(a^{(L)} - y) \\sigma'(z^{(L)}) \\\\\n",
    "    \\frac{\\partial C_i}{\\partial a_{j}^{(L-1)}} =&&\n",
    "        \\frac{\\partial C_i}{\\partial a^{(L)}}\n",
    "        \\frac{\\partial a^{(L)}}{\\partial z^{(L)}}\n",
    "        \\frac{\\partial z^{(L)}}{\\partial a^{(L-1)}}\n",
    "        &= 2(a^{(L)} - y) \\sigma'(z^{(L)}) w^{(L)}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-Wide Neural Network (L = 1)\n",
    "#### Two layered network\n",
    "\n",
    "Start with the simplest NN - a single input node connected to a single output node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def init_params():\n",
    "    w1 = np.random.randn(1) * 0.01\n",
    "    b1 = np.random.randn(1)\n",
    "    return w1, b1\n",
    "\n",
    "init_params()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
