{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of Gradient Descent and Neural Networks\n",
    "\n",
    "### Brendan Schlaman\n",
    "\n",
    "The purpose of this notebook is to incrementally translate the mathematics of\n",
    "gradient descent into code in the context of a simple neural network.\n",
    "\n",
    "We will start with the simplest possible neural network: a 1-wide ($m=1$), single layer ($L = 1$) network, and build up from there."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation\n",
    "\n",
    "The following conventions will be used:\n",
    "\n",
    "| Variable or symbol | Definition |\n",
    "|---|---|\n",
    "| $\\mathbf{w}$ | A single weights matrix that feeds into a hidden layer.  $w$ is an $(m \\times n)$ matrix, where $n$ is the width (size) of the inputs, and $m$ is the size of the outputs (the width of the hidden layer). |\n",
    "| `w1` | When using a fixed number of layers *L*, `w1` will be the weights feeding into the first hidden layer (or the output layer).  `wn` in the code corresponds to $w^{(n)}$ in the math notation. |\n",
    "| $\\mathbf{b}$ | A single bias matrix that contributes to a hidden layer.  $\\mathbf{b}$ is a $(m \\times 1)$ matrix, where $m$ is the size (width) of the associated hidden layer.  |\n",
    "| `b1` | When using a fixed number of layers *L1*, `b1` will be the weights feeding into the first hidden layer (or the output layer).  `bn` in the code corresponds to $b^{(n)}$ in the math notation. |\n",
    "| `z` | A preactivated layer.  It is the result of the linear combination (weights times inputs plus bias) before an activation function is applied. |\n",
    "| `a` | An activated layer, i.e. $\\sigma (z)$. |\n",
    "| $L$ | The depth of the neural network; the number of hidden layers plus the output layer. |\n",
    "| $C_k$ | The cost of the neural network for training example $k$. |\n",
    "\n",
    "\n",
    "\\*Note that when using a fixed number of layers *L > 1*, the *input layer* is treated as the “0th” layer and does not have wieghts or biases.\n",
    "Think of this like each layer owning the weights and biases that feed into it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key equations\n",
    "\n",
    "$z^{(L)} = w^{(L)}a^{(L-1)} + b^{(L)}$\n",
    "\n",
    "$a^{(L)} = \\sigma (z^{(L)})$\n",
    "\n",
    "We'll use the squared error loss function:\n",
    "\n",
    "$C_0(\\dots) = (a^{(L)} - y)^2$\n",
    "\n",
    "For a single layer, 1-wide network (with squared error loss):\n",
    "\n",
    "$\\frac{\\partial C_0}{\\partial w^{(L)}} = \\frac{\\partial z^{(L)}}{\\partial w^{(L)}} \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} \\frac{\\partial C_0}{\\partial a^{(L)}} = a^{(L-1)} \\sigma'(z^{(L)}) 2(a^{(L)} - y)$\n",
    "\n",
    "Over all examples $x_k$, the total loss partial derivative is\n",
    "\n",
    "$\\frac{\\partial C}{\\partial w^{(L)}} = \\frac{1}{n} \\sum_{k=0}^{n-1} \\frac{\\partial C_k}{\\partial w^{(L)}}$\n",
    "\n",
    "The classic softmax equation:\n",
    "\n",
    "$\\sigma(z)_j = \\frac{e^{z_j}}{\\sum_{k=1}^{K} e^{z_k}}$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: !Important!  Add a section describing how NN depth + width and training data size are reflected in the dimensionality of my datastructures\n",
    "\n",
    "What is `w[0]`, `w[0][0]`, `b[0]`, `z[0]`?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-Wide Neural Network (L = 1)\n",
    "#### Two layered network\n",
    "\n",
    "Start with the simplest NN - a single input node connected to a single output node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def init_params():\n",
    "    w1 = np.random.randn(1) * 0.01\n",
    "    b1 = np.random.randn(1)\n",
    "    return w1, b1\n",
    "\n",
    "init_params()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$asdf$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
