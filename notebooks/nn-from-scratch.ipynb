{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of Gradient Descent and Neural Networks\n",
    "\n",
    "### Brendan Schlaman\n",
    "\n",
    "**Dependencies:**\n",
    "- Jupyter (with MathJax support for LaTeX)\n",
    "- `numpy`\n",
    "\n",
    "The purpose of this notebook is to incrementally translate the mathematics of\n",
    "gradient descent into code in the context of a simple neural network.\n",
    "\n",
    "We will use only simple math libraries like `numpy` to gain a more fundamental intuition of the concepts; these concepts are\n",
    "abstracted away in more purpose-built libraries like `pytorch` or TensorFlow.\n",
    "\n",
    "We will start with the simplest possible neural network: a 1-wide ($m=1$), single layer ($L = 1$) network, and build up from there."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation\n",
    "\n",
    "The following conventions will be used:\n",
    "\n",
    "| Variable or symbol | Definition |\n",
    "|---|---|\n",
    "| $\\mathbf{w}$ | A single weights matrix that feeds into a hidden layer.  $w$ is an $(m \\times n)$ matrix, where $n$ is the width (size) of the inputs, and $m$ is the size of the outputs (the width of the hidden layer). |\n",
    "| `w1` | When using a fixed number of layers *L*, `w1` will be the weights feeding into the first hidden layer (or the output layer).  `wn` in the code corresponds to $w^{(n)}$ in the math notation. |\n",
    "| $\\mathbf{b}$ | A single bias matrix that contributes to a hidden layer.  $\\mathbf{b}$ is a $(m \\times 1)$ matrix, where $m$ is the size (width) of the associated hidden layer.  |\n",
    "| `b1` | When using a fixed number of layers *L1*, `b1` will be the weights feeding into the first hidden layer (or the output layer).  `bn` in the code corresponds to $b^{(n)}$ in the math notation. |\n",
    "| `z` | A preactivated layer.  It is the result of the linear combination (weights times inputs plus bias) before an activation function is applied. |\n",
    "| `a` | An activated layer, i.e. $\\sigma (z)$. |\n",
    "| $L$ | The depth of the neural network; the number of hidden layers plus the output layer. |\n",
    "| $C_k$ | The cost of the neural network for training example $k$. |\n",
    "\n",
    "\n",
    "> Note that when using a fixed number of layers *L > 1*, the *input layer* is treated as the “0th” layer and does not have wieghts or biases.\n",
    "Think of this like each layer owning the weights and biases that feed into it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key equations\n",
    "\n",
    "#### Neural network definitions\n",
    "\n",
    "$\\mathbf{z}^{(L)} = \\mathbf{w}^{(L)}\\mathbf{a}^{(L-1)} + \\mathbf{b}^{(L)}$\n",
    "\n",
    "$\\mathbf{a}^{(L)} = \\sigma (\\mathbf{z}^{(L)})$\n",
    "\n",
    "> Going forward, I'll drop the boldface notation for vectors and matrices.  $w$ is always a matrix, and $b$, $z$, are always vectors.\n",
    "\n",
    "#### Nonlinear activation functions\n",
    "\n",
    "The standard softmax function $\\sigma : \\mathbb{R}^K \\mapsto (0, 1)^K; \\; K \\geq 1$:\n",
    "\n",
    "$\\sigma(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}} \\text{ for } 1, \\dots, K \\text{ and } \\mathbf{z} = (z_1, \\dots, z_K) \\in \\mathbb{R}^K$\n",
    "\n",
    "#### Computing loss\n",
    "\n",
    "We'll use the **squared error loss function***:\n",
    "\n",
    "$C_0(\\dots) = (a^{(L)} - y)^2$\n",
    "\n",
    "For a single layer, 1-wide network (with squared error loss):\n",
    "\n",
    "$\\frac{\\partial C_0}{\\partial w^{(L)}} = \\frac{\\partial z^{(L)}}{\\partial w^{(L)}} \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} \\frac{\\partial C_0}{\\partial a^{(L)}} = a^{(L-1)} \\sigma'(z^{(L)}) 2(a^{(L)} - y)$\n",
    "\n",
    "The full gradient of the cost function over the entire network $\\nabla C$ comprises the above derivatives across all layers.\n",
    "\n",
    "$\n",
    "\\newcommand{\\arraystretch}{1.5}\n",
    "\\nabla C = \n",
    "    \\begin{bmatrix}\n",
    "    \\frac{\\partial C}{\\partial w^{(1)}} \\\\\n",
    "    \\frac{\\partial C}{\\partial b^{(1)}} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\partial C}{\\partial w^{(L)}} \\\\\n",
    "    \\frac{\\partial C}{\\partial b^{(L)}}\n",
    "    \\end{bmatrix}\n",
    "$\n",
    "\n",
    "Over all examples $x_k$, the total loss derivative is\n",
    "\n",
    "$\\frac{\\partial C}{\\partial w^{(L)}} = \\frac{1}{n} \\sum_{k=0}^{n-1} \\frac{\\partial C_k}{\\partial w^{(L)}}$\n",
    "\n",
    "\n",
    "Cross entropy loss:\n",
    "\n",
    "$C = -\\sum_{j=1}^{K} y_j \\log(\\hat{y}_j)$\n",
    "\n",
    "$C = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{K} y_{ij} \\log(\\hat{y}_{ij})$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-Wide Neural Network (L = 1)\n",
    "#### Two layered network\n",
    "\n",
    "Start with the simplest NN - a single input node connected to a single output node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def init_params():\n",
    "    w1 = np.random.randn(1) * 0.01\n",
    "    b1 = np.random.randn(1)\n",
    "    return w1, b1\n",
    "\n",
    "init_params()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
