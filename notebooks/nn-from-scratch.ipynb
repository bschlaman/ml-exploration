{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of Gradient Descent and Neural Networks\n",
    "\n",
    "#### Brendan Schlaman (2023.07.01)\n",
    "\n",
    "**Technical Requirements:**\n",
    "- Jupyter (with MathJax support for $\\LaTeX$)\n",
    "- `numpy`\n",
    "\n",
    "**Human Requirements:**\n",
    "- Reasonable understanding of linear algebra and calculus\n",
    "\n",
    "---\n",
    "\n",
    "### Introduction\n",
    "\n",
    "The purpose of this notebook is to incrementally translate the mathematics of\n",
    "gradient descent into code in the context of a simple neural network.\n",
    "\n",
    "We will use only simple math libraries like `numpy` to gain a more fundamental intuition of the concepts; these concepts are\n",
    "abstracted away in more purpose-built libraries like `pytorch` or TensorFlow.\n",
    "\n",
    "We will start with the simplest possible neural network: a 1-wide ($m=1$), single layer ($L = 1$) network, and build up from there."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation\n",
    "\n",
    "The following conventions will be used:\n",
    "\n",
    "| Variable or symbol | Definition |\n",
    "|---|---|\n",
    "| $\\mathbf{W}$ | A single weights matrix that feeds into a hidden layer.  $W$ is an $(m \\times n)$ matrix, where $n$ is the width (size) of the inputs, and $m$ is the size of the outputs (the width of the hidden layer). |\n",
    "| $\\mathbf{w}$ | Same as above, but when $w$ is a vector (single column matrix).  This is the case where there is a single neuron for a particular layer. |\n",
    "| `W1` | When using a fixed number of layers *L*, `W1` will be the weights feeding into the first hidden layer (or the output layer).  `Wn` in the code corresponds to $w^{(n)}$ in the math notation. |\n",
    "| $\\mathbf{b}$ | A single bias matrix that contributes to a hidden layer.  $\\mathbf{b}$ is a $(m \\times 1)$ matrix, where $m$ is the size (width) of the associated hidden layer.  |\n",
    "| `b1` | When using a fixed number of layers *L*, `b1` will be the weights feeding into the first hidden layer (or the output layer).  `bn` in the code corresponds to $b^{(n)}$ in the math notation. |\n",
    "| `z` | A preactivated layer.  It is the result of the linear combination (weights times inputs plus bias) before an activation function is applied. |\n",
    "| `a` | An activated layer, i.e. $\\sigma (z)$.  The activated layer becomes the input to the next layer. |\n",
    "| $L$ | The depth of the neural network; the number of hidden layers plus the output layer. |\n",
    "| $C_i$ | The cost of the neural network for training example $i$. |\n",
    "| $j$ | Index of the current layer, the layer denoted in the superscript. |\n",
    "| $k$ | Index of the previous layer, the layer before the layer denoted in the superscript. |\n",
    "\n",
    "<br>\n",
    "\n",
    "> Note that when using a fixed number of layers *L > 1*, the *input layer* is treated as the “0th” layer and does not have wieghts or biases.\n",
    "Think of this like each layer owning the weights and biases that feed into it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key equations\n",
    "\n",
    "#### Neural network definitions\n",
    "\n",
    "$$\n",
    "    \\mathbf{z}^{(L)} = \\mathbf{W}^{(L)}\\mathbf{a}^{(L-1)} + \\mathbf{b}^{(L)}\n",
    "$$\n",
    "$$\n",
    "    \\mathbf{a}^{(L)} = \\sigma (\\mathbf{z}^{(L)})\n",
    "$$\n",
    "\n",
    "> Going forward, I'll drop the boldface notation for vectors and matrices.  $W$ is always a matrix, and $b$, $z$, $w$ are always vectors.\n",
    "\n",
    "\n",
    "#### Computing loss\n",
    "\n",
    "We'll use the common **Mean Squared Error (MSE)** loss function:\n",
    "\n",
    "$$\n",
    "    C_i = \\frac{1}{n_L}\\sum_{j=0}^{n_L-1}(y_j - a_j^{(L)})^2\n",
    "$$\n",
    "\n",
    "where $C_i$ is the loss from a single example, $x_i$.\n",
    "This is precicely the function we will try to minimize with gradient descent.\n",
    "Note that $\\frac{1}{n_L}$ simply scales the loss and does not change which inputs minimize the function.\n",
    "\n",
    "We are interested in how the cost of the network changes with respect to it's inputs, $W$ and $b$.\n",
    "The chain rule gives us our answer:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial C_i}{\\partial W_{jk}^{(l)}} =\n",
    "        \\frac{\\partial C_i}{\\partial a_j^{(l)}}\n",
    "        \\frac{\\partial a_j^{(l)}}{\\partial z_j^{(l)}}\n",
    "        \\frac{\\partial z_j^{(l)}}{\\partial W_{jk}^{(l)}}\n",
    "$$\n",
    "$$\n",
    "    \\frac{\\partial C_i}{\\partial b_j^{(l)}} =\n",
    "        \\frac{\\partial C_i}{\\partial a_j^{(l)}}\n",
    "        \\frac{\\partial a_j^{(l)}}{\\partial z_j^{(l)}}\n",
    "        \\frac{\\partial z_j^{(l)}}{\\partial b_j^{(l)}}\n",
    "$$\n",
    "\n",
    "It's also helpful to consider the derivative of $C_i$ with respect to $a_j^{(l)}$,\n",
    "since through that derivative, we get access future layers' ($l + 1, 2, \\dots$) weights and biases.\n",
    "Notice that the cost function is influenced in $n_{l+1}$ different ways for each $a_j^{(l)}$,\n",
    "so we use the *multivariable chain rule* to account for all the paths between $a_j^{(l)}$ and the next layer.\n",
    "Remember that when context switches from $l$ to $l + 1$, the indices $j, k$ are redefined in relation to that new context.\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial C_i}{\\partial a_j^{(l)}} =\n",
    "        \\sum_{j=0}^{n_{l+1}-1}\n",
    "        \\frac{\\partial C_i}{\\partial a_j^{(l+1)}}\n",
    "        \\frac{\\partial a_j^{(l+1)}}{\\partial z_j^{(l+1)}}\n",
    "        \\frac{\\partial z_j^{(l+1)}}{\\partial a_j^{(l)}}\n",
    "$$\n",
    "\n",
    "The total loss derivative with respect to $W^{(L)}$ is the average of the losses $C_i$ from all examples $x_i$.\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial C}{\\partial W^{(L)}} = \\frac{1}{n} \\sum_{i=0}^{n-1} \\frac{\\partial C_i}{\\partial W^{(L)}}\n",
    "$$\n",
    "\n",
    "The full gradient of the loss function over the entire network $\\nabla C$ comprises the above derivatives for all layers $l$.\n",
    "\n",
    "$$\n",
    "\\nabla C =\n",
    "    \\begin{bmatrix}\n",
    "    \\frac{\\partial C}{\\partial W^{(1)}} \\\\[1.5ex]\n",
    "    \\frac{\\partial C}{\\partial b^{(1)}} \\\\[1.5ex]\n",
    "    \\vdots \\\\[1.5ex]\n",
    "    \\frac{\\partial C}{\\partial W^{(L)}} \\\\[1.5ex]\n",
    "    \\frac{\\partial C}{\\partial b^{(L)}}\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Cross entropy loss:\n",
    "\n",
    "$C = -\\sum_{j=1}^{K} y_j \\log(\\hat{y}_j)$\n",
    "\n",
    "$C = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{K} y_{ij} \\log(\\hat{y}_{ij})$\n",
    "\n",
    "#### Nonlinear activation functions\n",
    "\n",
    "The standard softmax function $\\sigma : \\mathbb{R}^K \\mapsto (0, 1)^K; \\; K \\geq 1$:\n",
    "\n",
    "$$\n",
    "    \\sigma(\\mathbf{z})_i =\n",
    "        \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n",
    "        \\text{ for }\n",
    "        1, \\dots, K\n",
    "        \\text{ and }\n",
    "        \\mathbf{z} = (z_1, \\dots, z_K) \\in \\mathbb{R}^K\n",
    "$$\n",
    "\n",
    "> The symbols used above are consistent with most online materials,\n",
    "> where $i$ represents an index of the elements of $\\mathbf{z}$,\n",
    "> not an example index.\n",
    "\n",
    "For this neural network, we'll be using **ReLU** as our activation function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simpler cases\n",
    "\n",
    "It's helpful to look at some of the math in edge-case neural networks.\n",
    "\n",
    "In the case of a single output, our preactivation function can be written as\n",
    "\n",
    "$$\n",
    "    z = f(\\mathbf{a}) = \\mathbf{w} \\cdot \\mathbf{a} + b\n",
    "$$\n",
    "\n",
    "For a single layer ($L = 1$), 1-wide ($m = 1$) network, the partial derivatives of loss are:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial C_i}{\\partial w^{(L)}} =&&\n",
    "        \\frac{\\partial C_i}{\\partial a^{(L)}}\n",
    "        \\frac{\\partial a^{(L)}}{\\partial z^{(L)}}\n",
    "        \\frac{\\partial z^{(L)}}{\\partial w^{(L)}}\n",
    "        &= 2(a^{(L)} - y) \\sigma'(z^{(L)}) a^{(L-1)} \\\\\n",
    "    \\frac{\\partial C_i}{\\partial b^{(L)}} =&&\n",
    "        \\frac{\\partial C_i}{\\partial a^{(L)}}\n",
    "        \\frac{\\partial a^{(L)}}{\\partial z^{(L)}}\n",
    "        \\frac{\\partial z^{(L)}}{\\partial b^{(L)}}\n",
    "        &= 2(a^{(L)} - y) \\sigma'(z^{(L)}) \\\\\n",
    "    \\frac{\\partial C_i}{\\partial a^{(L-1)}} =&&\n",
    "        \\frac{\\partial C_i}{\\partial a^{(L)}}\n",
    "        \\frac{\\partial a^{(L)}}{\\partial z^{(L)}}\n",
    "        \\frac{\\partial z^{(L)}}{\\partial a^{(L-1)}}\n",
    "        &= 2(a^{(L)} - y) \\sigma'(z^{(L)}) w^{(L)}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-Wide, single layer Neural Network ($L = 1$, $m^{(L)} = 1$)\n",
    "\n",
    "Start with the simplest NN - an input layer connected to a single output node (one category)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "\n",
    "First we'll define some utility functions that help us build the data for our example.\n",
    "The nature of these functions is chosen arbitrarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def init_single_category_data(d: int):\n",
    "    \"\"\"Generates random training data with dimensionality `d`.\n",
    "    The label is also chosen arbitrarily.\n",
    "    \"\"\"\n",
    "    data_size = 10\n",
    "    labels = [9] * data_size # randomly chose a desired label\n",
    "    examples = [[random.randrange(-9, 10) for _ in range(d)] for _ in range(data_size)]\n",
    "    return list(zip(examples, labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-7,  1,  7],\n",
       "        [ 9, -9, -3],\n",
       "        [ 0, -4, -1],\n",
       "        [ 5,  7,  9],\n",
       "        [-7,  0,  6],\n",
       "        [ 9, -8, -9],\n",
       "        [-2, -6, -8],\n",
       "        [ 4, -4, -3],\n",
       "        [-9, -2,  7],\n",
       "        [-7,  1,  5]]),\n",
       " array([9, 9, 9, 9, 9, 9, 9, 9, 9, 9]),\n",
       " array([-0.00644885, -0.01198742,  0.00288427]),\n",
       " array([-0.77612223]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_dimensionality = 3\n",
    "\n",
    "examples, labels = zip(*init_single_category_data(input_dimensionality))\n",
    "X, Y = np.array(examples), np.array(labels)\n",
    "\n",
    "m1 = 1 # the width of our only layer! (remember, we don't count the input layer)\n",
    "\n",
    "def init_params():\n",
    "    w1 = np.random.rand(input_dimensionality) * 0.01\n",
    "    b1 = np.random.randn(m1)\n",
    "    return w1, b1\n",
    "\n",
    "w1, b1 = init_params()\n",
    "\n",
    "X, Y, w1, b1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward propagation is simple.  Take the dot product between `w1` and `x` (remember, in this simple case, `w1` and `x` are vectors) and add `b1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(\n",
    "    x: np.ndarray,\n",
    "    w1: np.ndarray,\n",
    "    b1: np.ndarray,\n",
    "):\n",
    "    z1 = w1.dot(x) + b1\n",
    "    a1 = max(0, z1) # 1D ReLU\n",
    "    # print(w1, x, w1.dot(x), z1, a1)\n",
    "    return z1, a1\n",
    "\n",
    "def mean_squared_error_loss(label: int, oL: int) -> float:\n",
    "    return (label - oL)**2\n",
    "\n",
    "def run(X: np.ndarray, Y: np.ndarray):\n",
    "    w1, b1 = init_params()\n",
    "    print(w1, b1)\n",
    "    for _ in range(1):\n",
    "        losses = []\n",
    "        for (i, x) in enumerate(X):\n",
    "            _, a1 = forward_prop(x, w1, b1)\n",
    "            losses.append(mean_squared_error_loss(Y[i], a1))\n",
    "        avg_loss = sum(losses) / len(losses)\n",
    "        print(losses, avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01310992 -0.0128191  -0.00827641] [-0.20066038]\n",
      "[ 0.01310992 -0.0128191  -0.00827641] [-7  1  7] -0.16252340793702288 [-0.36318379] 0\n",
      "[ 0.01310992 -0.0128191  -0.00827641] [ 9 -9 -3] 0.2581904255536273 [0.05753004] [0.05753004]\n",
      "[ 0.01310992 -0.0128191  -0.00827641] [ 0 -4 -1] 0.05955281037706585 [-0.14110757] 0\n",
      "[ 0.01310992 -0.0128191  -0.00827641] [5 7 9] -0.0986717655888442 [-0.29933215] 0\n",
      "[ 0.01310992 -0.0128191  -0.00827641] [-7  0  6] -0.14142789949615908 [-0.34208828] 0\n",
      "[ 0.01310992 -0.0128191  -0.00827641] [ 9 -8 -9] 0.2950297716810054 [0.09436939] [0.09436939]\n",
      "[ 0.01310992 -0.0128191  -0.00827641] [-2 -6 -8] 0.11690602260085764 [-0.08375436] 0\n",
      "[ 0.01310992 -0.0128191  -0.00827641] [ 4 -4 -3] 0.12854531323849513 [-0.07211507] 0\n",
      "[ 0.01310992 -0.0128191  -0.00827641] [-9 -2  7] -0.15028594963607236 [-0.35094633] 0\n",
      "[ 0.01310992 -0.0128191  -0.00827641] [-7  1  5] -0.1459705923460966 [-0.34663097] 0\n",
      "[81, array([79.96776891]), 81, 81, 81, array([79.31025655]), 81, 81, 81, 81] [80.72780255]\n"
     ]
    }
   ],
   "source": [
    "run(X, Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the same `ReLU` function throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(Z: np.ndarray) -> np.ndarray:\n",
    "    return np.maximum(0, Z)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A beautiful intuition behind the Jacobian\n",
    "\n",
    "I had a beautifule \"eureka\" moment today.  I understand the motivation behind the **numerator layout** of the Jacobian matrix.\n",
    "As a reminder, the Jacobian is defined as follows:\n",
    "\n",
    "$$\n",
    "    \\mathbf{J} \\mathbf{f}(\\mathbf{x}) = \\nabla \\mathbf{f}(\\mathbf{x}) = \n",
    "    \\begin{bmatrix}\n",
    "    \\nabla f_1(\\mathbf{x}) \\\\[1.5ex]\n",
    "    \\nabla f_2(\\mathbf{x}) \\\\[1.5ex]\n",
    "    \\nabla f_3(\\mathbf{x}) \\\\[1.5ex]\n",
    "    \\vdots \\\\[1.5ex]\n",
    "    \\nabla f_i(\\mathbf{x})\n",
    "    \\end{bmatrix}\n",
    "=\n",
    "    \\begin{bmatrix}\n",
    "    \\frac{\\partial}{\\partial \\mathbf{x}} f_1(\\mathbf{x}) \\\\[1.5ex]\n",
    "    \\frac{\\partial}{\\partial \\mathbf{x}} f_2(\\mathbf{x}) \\\\[1.5ex]\n",
    "    \\frac{\\partial}{\\partial \\mathbf{x}} f_3(\\mathbf{x}) \\\\[1.5ex]\n",
    "    \\vdots \\\\[1.5ex]\n",
    "    \\frac{\\partial}{\\partial \\mathbf{x}} f_i(\\mathbf{x})\n",
    "    \\end{bmatrix}\n",
    "= \n",
    "    \\begin{bmatrix}\n",
    "    \\frac{\\partial}{\\partial x_1} f_1(\\mathbf{x}) \\frac{\\partial}{\\partial x_2} f_1(\\mathbf{x}) \\dots \\frac{\\partial}{\\partial x_j} f_1(\\mathbf{x}) \\\\[1.5ex]\n",
    "    \\frac{\\partial}{\\partial x_1} f_2(\\mathbf{x}) \\frac{\\partial}{\\partial x_2} f_2(\\mathbf{x}) \\dots \\frac{\\partial}{\\partial x_j} f_2(\\mathbf{x}) \\\\[1.5ex]\n",
    "    \\frac{\\partial}{\\partial x_1} f_3(\\mathbf{x}) \\frac{\\partial}{\\partial x_2} f_3(\\mathbf{x}) \\dots \\frac{\\partial}{\\partial x_j} f_3(\\mathbf{x}) \\\\[1.5ex]\n",
    "    \\vdots \\\\[1.5ex]\n",
    "    \\frac{\\partial}{\\partial x_1} f_i(\\mathbf{x}) \\frac{\\partial}{\\partial x_2} f_i(\\mathbf{x}) \\dots \\frac{\\partial}{\\partial x_j} f_i(\\mathbf{x})\n",
    "    \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
