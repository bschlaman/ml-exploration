{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of Gradient Descent and Neural Networks\n",
    "\n",
    "The purpose of this notebook is to translate the mathematical ideas in\n",
    "gradient descent into code.\n",
    "\n",
    "We start with a 1-wide neural network and build up from there."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation\n",
    "\n",
    "The following conventions will be used:\n",
    "\n",
    "| Variable or symbol | Definition |\n",
    "|---|---|\n",
    "| *w* | A single weights matrix that feeds into a hidden layer.  `w` is an `(m, n)` matrix, where *n* is the width (size) of the inputs, and *m* is the size of the outputs (the width of the hidden layer). |\n",
    "| `w1` | When using a fixed number of layers *L*, `w1` will be the weights feeding into the first hidden layer. |\n",
    "| *b* | A single bias matrix that contributes to a hidden layer.  `b` is a `(m, 1)` matrix, where *m* is the size (width) of the associated hidden layer.  |\n",
    "| `b1` | When using a fixed number of layers *L1*, `b1` will be the weights feeding into the first hidden layer. |\n",
    "| `z` | A preactivated layer.  It is the result of the linear combination (weights times inputs plus bias) before an activation function is applied. |\n",
    "| `a` | An activated layer, i.e. `f(z)`. |\n",
    "| *L* | The depth of the neural network; the number of hidden layers plus the output layer. |\n",
    "\n",
    "\n",
    "Note that when using a fixed number of layers *L > 1*, the *input layer* is treated as the “0th” layer and does not have wieghts or biases.\n",
    "Think of this like each layer owning the weights and biases that feed into it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: !Important!  Add a section describing how NN depth + width and training data size are reflected in the dimensionality of my datastructures\n",
    "\n",
    "What is `w[0]`, `w[0][0]`, `b[0]`, `z[0]`?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-Wide Neural Network (L = 1)\n",
    "#### Two layered network\n",
    "\n",
    "Start with the simplest NN - a single input node connected to a single output node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.11042608])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def init_params():\n",
    "    w1 = np.random.randn(1) * 0.01\n",
    "    b1 = np.random.randn(1)\n",
    "    return w1, b1\n",
    "\n",
    "init_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
