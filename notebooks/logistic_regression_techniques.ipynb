{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression techniques\n",
    "\n",
    "When working with logistic regression, I have seen\n",
    "algorithms where $y_i \\in \\{-1, 1\\}$, and others\n",
    "where $y_i \\in \\{0, 1\\}$.  This leads to slightly\n",
    "different sets of equations, and indeed in code,\n",
    "there seems to be a difference in performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def num_incorrect(labels, predictions):\n",
    "    c1 = (predictions > 0.5) & (labels == 1)\n",
    "    # accounting for both methods here\n",
    "    c2 = (predictions < 0.5) & (labels == 0 | labels == -1)\n",
    "\n",
    "    return np.count_nonzero(~np.logical_or(c1, c2))\n",
    "\n",
    "\n",
    "def sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def init_params(num_features: int) -> np.ndarray:\n",
    "    return np.random.randn(num_features)\n",
    "\n",
    "\n",
    "alpha = 0.01\n",
    "grad_desc_cycles = 100\n",
    "X_train, Y_train = np.zeros((5, 10)), np.ones(5)\n",
    "w = init_params(len(X_train))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1\n",
    "\n",
    "Case: $y_i \\in \\{0, 1\\}$\n",
    "\n",
    "Use gradient descent on the actual errors;\n",
    "I think this is the cleaner approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(grad_desc_cycles):\n",
    "    z = w.dot(X_train)\n",
    "    predictions = sigmoid(z)\n",
    "    errors = Y_train - predictions\n",
    "    grad = errors.dot(X_train.T) / len(Y_train)\n",
    "    w = w + alpha * grad\n",
    "\n",
    "    if i % 10: continue\n",
    "    print(np.count_nonzero(~num_incorrect(Y_train, predictions)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: MLE Negative Log Likelihood\n",
    "\n",
    "Case: $y_i \\in \\{-1, 1\\}$\n",
    "\n",
    "#### MLE\n",
    "\n",
    "Choose parameters that maximize the conditional likelihood.\n",
    "The conditional data likelihood $P(\\mathbf{y} | X, \\mathbf{w})$\n",
    "is the probability of the observed values $\\mathbf{y} \\in \\mathbb{R}^n$\n",
    "in the training data conditioned on the feature values $\\mathbf{x}_i$.\n",
    "Note that $X = [\\mathbf{x}_1,\\dots,\\mathbf{x}_n] \\in \\mathbb{R}^{d \\times n}$.\n",
    "We choose the parameters that maximize this function, and we assume that\n",
    "the $y_i$ are independent given the input features $\\mathbf{x}_i$ and $\\mathbf{w}$.\n",
    "\n",
    "$$\n",
    "    P(\\mathbf{y} | X, \\mathbf{w}) = \\prod_{i=1}^{n} P(y_i | \\mathbf{x}_i, \\mathbf{w}) \\\\\n",
    "    \\hat{\\mathbf{w}}_{\\text{MLE}}\n",
    "    = \\underset{\\mathbf{w}}{\\arg\\max}\n",
    "    - \\sum_{i=1}^{n}\\log(1 + e^{-y_i\\mathbf{w}^T\\mathbf{x}_i}) \\\\\n",
    "    = \\underset{\\mathbf{w}}{\\arg\\min} \\sum_{i=1}^{n}\\log(1 + e^{-y_i\\mathbf{w}^T\\mathbf{x}_i})\n",
    "$$\n",
    "\n",
    "Use gradient descent on the _negative log likelihood_.\n",
    "\n",
    "$$\n",
    "    \\ell(\\mathbf{w}) = \\sum_{i=1}^{n}\\log(1 + e^{-y_i\\mathbf{w}^T\\mathbf{x}_i})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(Y: np.ndarray, z: np.ndarray):\n",
    "    return np.log(1 + np.exp(-Y * z)) / len(Y)\n",
    "\n",
    "\n",
    "def grad(Y, w, X):\n",
    "    # The commented out return statements are all the same;\n",
    "    # you can see the simplifications as I figured out\n",
    "    # they were equivalent.\n",
    "    # return np.sum(Y * (sigmoid(w.dot(X)) - 1) * X, axis=1) / len(Y)\n",
    "    # return (Y * (sigmoid(w.dot(X)) - 1)).dot(X.T) / len(Y)\n",
    "    # return sigmoid(Y * w.dot(X)).dot(X.T) / len(Y)\n",
    "    z = w.dot(X)\n",
    "    return sigmoid(Y * z).dot(X.T) / len(Y)\n",
    "\n",
    "\n",
    "for i in range(grad_desc_cycles):\n",
    "    w = w - alpha * grad(Y_train, w, X_train)\n",
    "\n",
    "    if i % 10: continue\n",
    "    predictions = sigmoid(w.dot(X_train))\n",
    "    print(np.count_nonzero(~num_incorrect(Y_train, predictions)))\n",
    "    print(np.sum(loss(Y_train, z)), np.around(w, 3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
